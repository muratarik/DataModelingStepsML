{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0\"></a>\n",
    "# **Feature Selection for Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel is based on Soledad Galli's course - [Feature Selection for Machine Learning](https://www.udemy.com/course/feature-selection-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature selection**\n",
    "\n",
    "- **Feature selection** or **variable selection** is the process of selecting a subset of relevant features or variables from the total features of a level in a data set to build machine learning algorithms. \n",
    "\n",
    "\n",
    "### **Advantages of selecting features**\n",
    "\n",
    "- There are various advantages of feature selection process. These are as follows:-\n",
    "\n",
    "  1.\tImproved accuracy\n",
    "  2.    Simple models are easier to interpret.\n",
    "  3.\tShorter training times\n",
    "  4.\tEnhanced generalization by reducing Overfitting\n",
    "  5.\tEasier to implement by software developers\n",
    "  6.\tReduced risk of data errors by model use\n",
    "  7.\tVariable redundancy\n",
    "  8.\tBad learning behaviour in high dimensional spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "# **Table of Contents**\n",
    "\n",
    "1.\t[Filter Methods](#1)\n",
    "   - 1.1. [Basic Methods](#1.1)\n",
    "        - 1.1.1 [Remove Constant Features](#1.1.1)\n",
    "        - 1.1.2 [Remove Quasi-Constant Features](#1.1.2)\n",
    "   - 1.2 [Univariate Selection Methods](#1.2)\n",
    "        - 1.2.1 [SelectKBest](#1.2.1)\n",
    "        - 1.2.2 [SelectPercentile](#1.2.2)\n",
    "   - 1.3 [Information Gain](#1.3)\n",
    "   - 1.4 [Fisher Score (chi-square implementation)](#1.4)\n",
    "   - 1.5 [ANOVA F-Value for Feature Selection](#1.5)\n",
    "   - 1.6 [Correlation-Matrix with Heatmap](#1.6)\n",
    "2.\t[Wrapper Methods](#2)\n",
    "\n",
    "   - 2.1. [Sequential Feature Selection](#2.1)\n",
    "       - 2.1.1 [Sequential Forward Selection (SFS)](#2.1.1)\n",
    "       - 2.1.2 [Sequential Backward Elimination (SBS)](#2.1.2)\n",
    "       - 2.1.3 [Sequential Floating Selection (SFFS and SFBS)](#2.1.3)\n",
    "       - 2.1.4 [Bidirectional Selection](#2.1.4)\n",
    "       - 2.1.5 [Exhaustive Feature Selection](#2.1.5)\n",
    "       - 2.1.6 [Limitations of Step Forward/Backward Selection](#2.1.6)\n",
    "       - 2.1.7 [Difference between filter and wrapper methods](#2.1.7)\n",
    "   - 2.2 [Recursive Feature Selection](#2.2)\n",
    "       - 2.2.1 [Recursive Feature Elimination](#2.2.1)\n",
    "       - 2.2.2 [Recursive Feature Elimination with Cross-Validation](#2.2.2)\n",
    "   \n",
    "3. [Embedded Methods](#3)\n",
    "   - 3.1 [LASSO Regression](#3.1)\n",
    "   - 3.2 [Random Forest Importance](#3.2)\n",
    "4. [How to choose the right feature selection method](#4)\n",
    "5. [Tips and tricks for feature selection](#5)   \n",
    "6. [References](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Filter Methods** <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The characteristics of these methods are as follows:-\n",
    "\n",
    "  - These methods rely on the characteristics of the data (feature characteristics)\n",
    "  - They do not use machine learning algorithms.\n",
    "  - These are model agnostic.\n",
    "  - They tend to be less computationally expensive.\n",
    "  - They usually give lower prediction performance than wrapper methods.\n",
    "  - They are very well suited for a quick screen and removal of irrelevant features.\n",
    "  \n",
    "  \n",
    " - Filter methods consists of various techniques as given below:-\n",
    "\n",
    "   -  1.1.    Basic methods\n",
    "   -  1.2.    Univariate feature selection\n",
    "   -  1.3.    Information gain\n",
    "   -  1.4.    Fischer score\n",
    "   -  1.5.    ANOVA F-Value for Feature Selection\n",
    "   -  1.6.    Correlation Matrix with Heatmap  \n",
    "  \n",
    "- Filter methods can be explained with the help of following graphic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FilterMethods](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537552825/Image3_fqsh79.png)\n",
    "\n",
    "\n",
    "### Image source : AnalyticsVidhya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Basic methods** <a class=\"anchor\" id=\"1.1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Under basic methods, we remove constant and quasi-constant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape : (76020, 370) Test shape : (75818, 370) y_train shape : (76020,)\n"
     ]
    }
   ],
   "source": [
    "def load_santander():\n",
    "    # import the Santander customer satisfaction dataset from Kaggle\n",
    "    X_train = pd.read_csv('dataset/train.csv')\n",
    "    y_train = X_train.TARGET\n",
    "    X_train = X_train.drop(columns=['TARGET'])\n",
    "\n",
    "    X_test = pd.read_csv('dataset/test.csv')\n",
    "    \n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "X_train, y_train, X_test = load_santander()\n",
    "# check shape of training and test sets\n",
    "print(f'training shape : {X_train.shape} Test shape : {X_test.shape} y_train shape : {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1.1 Remove constant features** <a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "- Constant features are those that show the same value, just one value, for all the observations of the dataset. This is, the same value for all the rows of the dataset. These features provide no information that allows a machine learning model to discriminate or predict a target.\n",
    "\n",
    "- Identifying and removing constant features, is an easy first step towards feature selection and more easily interpretable machine learning models. To identify constant features, we can use the VarianceThreshold function from sklearn.\n",
    "\n",
    "- I will demonstrate how to identify constant features using the Santander Customer Satisfaction dataset from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using variance threshold from sklearn**\n",
    "\n",
    "- Variance threshold from sklearn is a simple baseline approach to feature selection. It removes all features which variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VarianceThreshold(threshold=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sel = VarianceThreshold(threshold=0)\n",
    "sel.fit(X_train)  # fit finds the features with zero variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False, False, False, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True, False, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False, False, False,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "       False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It shows what feature are selected!\n",
    "sum(sel.get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that there are 51 columns / variables that are constant. This means that 51 variables show the same value, just one value, for all the observations of the training set.\n",
    "- We can see how by removing constant features, we managed to reduced the feature space quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then drop these columns from the train and test sets\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76020, 336), (75818, 336))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of training and test set\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ind_var2_0', 'ind_var2', 'ind_var27_0', 'ind_var28_0', 'ind_var28',\n",
       "       'ind_var27', 'ind_var41', 'ind_var46_0', 'ind_var46', 'num_var27_0',\n",
       "       'num_var28_0', 'num_var28', 'num_var27', 'num_var41', 'num_var46_0',\n",
       "       'num_var46', 'saldo_var28', 'saldo_var27', 'saldo_var41', 'saldo_var46',\n",
       "       'imp_amort_var18_hace3', 'imp_amort_var34_hace3',\n",
       "       'imp_reemb_var13_hace3', 'imp_reemb_var33_hace3',\n",
       "       'imp_trasp_var17_out_hace3', 'imp_trasp_var33_out_hace3',\n",
       "       'num_var2_0_ult1', 'num_var2_ult1', 'num_reemb_var13_hace3',\n",
       "       'num_reemb_var33_hace3', 'num_trasp_var17_out_hace3',\n",
       "       'num_trasp_var33_out_hace3', 'saldo_var2_ult1',\n",
       "       'saldo_medio_var13_medio_hace3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns[~sel.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1167a4750>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train['ind_var2_0'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "for i in X_train.columns[~sel.get_support()]:\n",
    "    print(X_train[i].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1.2 Remove quasi-constant features** <a class=\"anchor\" id=\"1.1.2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "- Quasi-constant features are those that show the same value for the great majority of the observations of the dataset. In general, these features provide little if any information that allows a machine learning model to discriminate or predict a target. But there can be exceptions. So we should be careful when removing these type of features. Identifying and removing quasi-constant features, is an easy first step towards feature selection and more easily interpretable machine learning models.\n",
    "\n",
    "- To identify quasi-constant features, we can once again use the VarianceThreshold function from sklearn.\n",
    "\n",
    "- Here I will demonstrate how to identify quasi-constant features using the Santander Customer Satisfaction dataset.\n",
    "\n",
    "- This method can be used for un-supervised learning since it does not use label for elimination!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing quasi-constant features**\n",
    "\n",
    "#### **Using variance threshold from sklearn**\n",
    "\n",
    "- Variance threshold from sklearn is a simple baseline approach to feature selection. It removes all features which variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples.\n",
    "\n",
    "- Here, I will change the default threshold to remove almost / quasi-constant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_trian, X_test = load_santander()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VarianceThreshold(threshold=0.01)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = VarianceThreshold(threshold=0.01)  # 0.1 indicates 99% of observations approximately\n",
    "\n",
    "sel.fit(X_train)  # fit finds the features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_support is a boolean vector that indicates which features \n",
    "# are retained. If we sum over get_support, we get the number\n",
    "# of features that are not quasi-constant\n",
    "sum(sel.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'var3', 'var15', 'imp_ent_var16_ult1', 'imp_op_var39_comer_ult1',\n",
       "       'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1',\n",
       "       'imp_op_var40_comer_ult3', 'imp_op_var40_efect_ult1',\n",
       "       'imp_op_var40_efect_ult3',\n",
       "       ...\n",
       "       'saldo_medio_var29_ult3', 'saldo_medio_var33_hace2',\n",
       "       'saldo_medio_var33_hace3', 'saldo_medio_var33_ult1',\n",
       "       'saldo_medio_var33_ult3', 'saldo_medio_var44_hace2',\n",
       "       'saldo_medio_var44_hace3', 'saldo_medio_var44_ult1',\n",
       "       'saldo_medio_var44_ult3', 'var38'],\n",
       "      dtype='object', length=273)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of features!\n",
    "X_train.columns[sel.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that 107 columns / variables are almost constant. This means that 107 variables show predominantly one value for ~99% the observations of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    75741\n",
       "1      279\n",
       "Name: ind_var31, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['ind_var31'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.99633\n",
       "1    0.00367\n",
       "Name: ind_var31, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of observations showing each of the different values\n",
    "X_train['ind_var31'].value_counts() / np.float(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that > 99% of the observations show one value, 0. Therefore, this feature is almost constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then remove the features from training and test set\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76020, 273), (75818, 273))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of training and test set\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By removing constant and quasi-constant features, we reduced the feature space from 370 to 263. We can see that more than 100 features were removed from the present dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 Univariate selection methods** <a class=\"anchor\" id=\"1.2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Univariate feature selection methods works by selecting the best features based on univariate statistical tests like ANOVA. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the transform method.\n",
    "\n",
    "- The methods based on F-test estimate the degree of linear dependency between two random variables. They assume a linear relationship between the feature and the target. These methods also assume that the variables follow a Gaussian distribution.\n",
    "\n",
    "- There are 4 methods that fall under this category :-\n",
    "\n",
    "  1. SelectKBest\n",
    "  2. SelectPercentile\n",
    "  3. SelectFpr, SelectFdr, or family wise error SelectFwe\n",
    "  4. GenericUnivariateSelection\n",
    "  \n",
    "Source : https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "\n",
    "- Here, I will limit the discussion to SelectKBest and SelectPercentile, because these two are most commonly used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2.1 SelectKBest** <a class=\"anchor\" id=\"1.2.1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "- This method select features according to the k highest scores.\n",
    "\n",
    "- For instance, we can perform a chi-square test to the samples to retrieve only the two best features from iris dataset as follows:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "X_train, y_trian, X_test = load_santander()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the two best features\n",
    "sel_kbest = SelectKBest(f_classif, k=5)\n",
    "X_new = sel_kbest.fit_transform(X_train, y_train)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_kbest.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ind_var5', 'ind_var30', 'num_var30', 'num_var42',\n",
       "       'num_meses_var5_ult3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selected best k Features\n",
    "X_train.columns[sel_kbest.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2.2 SelectPercentile** <a class=\"anchor\" id=\"1.2.2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Select features according to a percentile of the highest scores.\n",
    "\n",
    "Source : https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, chi2, f_classif, mutual_info_classif\n",
    "X_train, y_trian, X_test = load_santander()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 74)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now select features based on top 10 percentile\n",
    "sel_percent = SelectPercentile(f_classif, percentile=20)\n",
    "X_new = sel_percent.fit_transform(X_train, y_train)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that only 37 features lie on the top 10 percentile and hence we select them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var15', 'imp_op_var40_efect_ult3', 'imp_op_var41_efect_ult1',\n",
       "       'imp_op_var41_efect_ult3', 'imp_op_var41_ult1',\n",
       "       'imp_op_var39_efect_ult1', 'imp_op_var39_efect_ult3',\n",
       "       'imp_op_var39_ult1', 'ind_var5_0', 'ind_var5', 'ind_var8_0', 'ind_var8',\n",
       "       'ind_var12_0', 'ind_var12', 'ind_var13_0', 'ind_var13_corto_0',\n",
       "       'ind_var13_corto', 'ind_var13_largo_0', 'ind_var13', 'ind_var14_0',\n",
       "       'ind_var24_0', 'ind_var24', 'ind_var25_cte', 'ind_var26_cte',\n",
       "       'ind_var25_0', 'ind_var25', 'ind_var30', 'ind_var39_0', 'ind_var41_0',\n",
       "       'num_var4', 'num_var5_0', 'num_var5', 'num_var8_0', 'num_var8',\n",
       "       'num_var12_0', 'num_var12', 'num_var13_0', 'num_var13_corto_0',\n",
       "       'num_var13_corto', 'num_var13', 'num_var24_0', 'num_var24',\n",
       "       'num_var30_0', 'num_var30', 'num_var35', 'num_var39_0', 'num_var41_0',\n",
       "       'num_var42', 'saldo_var12', 'saldo_var13_corto', 'saldo_var13',\n",
       "       'saldo_var24', 'saldo_var30', 'saldo_var42', 'var36',\n",
       "       'imp_aport_var13_hace3', 'ind_var43_recib_ult1',\n",
       "       'num_aport_var13_hace3', 'num_var22_ult1', 'num_meses_var5_ult3',\n",
       "       'num_meses_var8_ult3', 'num_meses_var12_ult3',\n",
       "       'num_meses_var13_corto_ult3', 'num_op_var41_efect_ult1',\n",
       "       'num_op_var41_efect_ult3', 'num_op_var39_efect_ult1',\n",
       "       'num_op_var39_efect_ult3', 'saldo_medio_var5_hace2',\n",
       "       'saldo_medio_var12_ult1', 'saldo_medio_var12_ult3',\n",
       "       'saldo_medio_var13_corto_hace2', 'saldo_medio_var13_corto_ult1',\n",
       "       'saldo_medio_var13_corto_ult3', 'var38'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selected best 10 percentile Features\n",
    "X_train.columns[sel_percent.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Important information**\n",
    "\n",
    "- These objects take as input a scoring function that returns univariate scores and p-values (or only scores for [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) and [SelectPercentile](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile):\n",
    "\n",
    "\n",
    "- For regression tasks: [f_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression), [mutual_info_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression)\n",
    "\n",
    "- For classification tasks: [chi2](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2), \n",
    "[f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif), [mutual_info_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif)\n",
    "\n",
    "The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature selection with sparse data**\n",
    "\n",
    "- If you use sparse data (i.e. data represented as sparse matrices), [chi2](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2), [mutual_info_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression), [mutual_info_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif) will deal with the data without making it dense.\n",
    "\n",
    "\n",
    "Source : https://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Warning** \n",
    "- Beware not to use a regression scoring function with a classification problem, you will get useless results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 Information Gain** <a class=\"anchor\" id=\"1.3\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **Information gain** or **mutual information** measures how much information the presence/absence of a feature contributes to making the correct prediction on the target.\n",
    "\n",
    "\n",
    "- In terms of  [wikipedia](https://en.wikipedia.org/wiki/Mutual_information):\n",
    "\n",
    "\n",
    "   - **Mutual information measures the information that X and Y share: It measures how much knowing one of these        variables reduces uncertainty about the other. For example, if X and Y are independent, then knowing X does        not give any information about Y and vice versa, so their mutual information is zero. At the other extreme, if      X is a deterministic function of Y and Y is a deterministic function of X then all information conveyed by X        is shared with Y: knowing X determines the value of Y and vice versa. As a result, in this case the mutual          information is the same as the uncertainty contained in Y (or X) alone, namely the entropy of Y (or X).            Moreover, this mutual information is the same as the entropy of X and as the entropy of Y. (A very special          case of this is when X and Y are the same random variable.)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **mutual_info_classif**\n",
    "\n",
    "\n",
    "- It estimates mutual information for a discrete target variable.\n",
    "\n",
    "- Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency. \n",
    "\n",
    "- This function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\n",
    "\n",
    "- It can be used for univariate features selection.\n",
    "\n",
    "- Source : \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **mutual_info_regression**\n",
    "\n",
    "- Estimate mutual information for a continuous target variable.\n",
    "\n",
    "- Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n",
    "\n",
    "- The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\n",
    "\n",
    "- It can be used for univariate features selection\n",
    "\n",
    "- Source : \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.4 Fisher Score (chi-square implementation)** <a class=\"anchor\" id=\"1.4\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "- It is the **chi-square implementation** in scikit-learn. It computes chi-squared stats between each non-negative feature and class.\n",
    "\n",
    "- This score should be used to evaluate categorical variables in a classification task. It compares the observed distribution of the different classes of target Y among the different categories of the feature, against the expected distribution of the target classes, regardless of the feature categories. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "# create features and target\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# convert to categorical data by converting data to integers\n",
    "X = X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Chi-Squared Statistics\n",
    "# select two features with highest chi-squared statistics\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "X_kbest = chi2_selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 2\n"
     ]
    }
   ],
   "source": [
    "# View results\n",
    "print('Original number of features:', X.shape[1])\n",
    "print('Reduced number of features:', X_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the above code helps us to select the 2 best features based on Fisher score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.5 ANOVA F-value For Feature Selection**  <a class=\"anchor\" id=\"1.5\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Compute the ANOVA F-value for the provided sample.\n",
    "\n",
    "- If the features are categorical, we will calculate a chi-square statistic between each feature and the target vector. \n",
    "However, if the features are quantitative, we will compute the ANOVA F-value between each feature and the target vector.\n",
    "\n",
    "- The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "X_train, y_trian, X_test = load_santander()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the two best features\n",
    "sel_kbest = SelectKBest(f_classif, k=15)\n",
    "X_new = sel_kbest.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sel_kbest.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 370\n",
      "Reduced number of features: 15\n"
     ]
    }
   ],
   "source": [
    "# View results\n",
    "print('Original number of features:', X_train.shape[1])\n",
    "print('Reduced number of features:', X_new.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.6 Correlation-Matrix with Heatmap** <a class=\"anchor\" id=\"1.6\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **Correlation** is a measure of the linear relationship of 2 or more variables. Through correlation, we can predict one variable from the other.\n",
    "\n",
    "- **Good variables are highly correlated with the target**.\n",
    "\n",
    "- Correlated predictor variables provide redundant information.\n",
    "\n",
    "- **Variables should be correlated with the target but uncorrelated among themselves**.\n",
    "\n",
    "\n",
    "- Correlation Feature Selection evaluates subsets of features on the basis of the following hypothesis: \n",
    "\n",
    "   - \"Good feature subsets contain features highly correlated with the target, yet uncorrelated to each other\".\n",
    "   \n",
    "   \n",
    "- In this section,  I will demonstrate how to select features based on correlation between two features. We can find features that are correlated with each other. By identifying these features, we can then decide which features we want to keep, and which ones we want to remove.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using Pearson correlation our returned coefficient values will vary between -1 and 1.\n",
    "\n",
    "- If the correlation between two features is 0 this means that changing any of these two features will not affect the other.\n",
    "\n",
    "- If the correlation between two features is greater than 0 this means that increasing the values in one feature will make increase also the values in the other feature (the closer the correlation coefficient is to 1 and the stronger is going to be this bond between the two different features).\n",
    "\n",
    "- If the correlation between two features is less than 0 this means that increasing the values in one feature will make decrease the values in the other feature (the closer the correlation coefficient is to -1 and the stronger is going to be this relationship between the two different features).\n",
    "\n",
    "- In this analysis we will check if the selected variables are highly correlated with each other. If they are, we would then need to keep just one of the correlated ones and drop the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "X_train, y_trian, X_test = load_santander()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var29_ult3</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001025</td>\n",
       "      <td>-0.002135</td>\n",
       "      <td>-0.003111</td>\n",
       "      <td>-0.001436</td>\n",
       "      <td>-0.004131</td>\n",
       "      <td>-0.007277</td>\n",
       "      <td>-0.006302</td>\n",
       "      <td>-0.006700</td>\n",
       "      <td>-0.006698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007631</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.003771</td>\n",
       "      <td>-0.001521</td>\n",
       "      <td>-0.001216</td>\n",
       "      <td>-0.003772</td>\n",
       "      <td>-0.003674</td>\n",
       "      <td>-0.000856</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>-0.005687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var3</th>\n",
       "      <td>-0.001025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004452</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var15</th>\n",
       "      <td>-0.002135</td>\n",
       "      <td>-0.004452</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043722</td>\n",
       "      <td>0.094762</td>\n",
       "      <td>0.101177</td>\n",
       "      <td>0.042754</td>\n",
       "      <td>0.048512</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.009678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011623</td>\n",
       "      <td>0.029358</td>\n",
       "      <td>0.017264</td>\n",
       "      <td>0.028504</td>\n",
       "      <td>0.029176</td>\n",
       "      <td>0.029180</td>\n",
       "      <td>0.018884</td>\n",
       "      <td>0.032833</td>\n",
       "      <td>0.033597</td>\n",
       "      <td>0.006497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <td>-0.003111</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.043722</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>0.034879</td>\n",
       "      <td>0.009896</td>\n",
       "      <td>0.009377</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.002510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>-0.000632</td>\n",
       "      <td>-0.000548</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>0.002655</td>\n",
       "      <td>-0.000612</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.006590</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <td>-0.001436</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>0.094762</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886476</td>\n",
       "      <td>0.342709</td>\n",
       "      <td>0.295295</td>\n",
       "      <td>0.032280</td>\n",
       "      <td>0.054809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.016422</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.012570</td>\n",
       "      <td>0.013703</td>\n",
       "      <td>0.009445</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.011665</td>\n",
       "      <td>0.010802</td>\n",
       "      <td>0.012546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <td>-0.003772</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.029180</td>\n",
       "      <td>0.002655</td>\n",
       "      <td>0.009445</td>\n",
       "      <td>0.008539</td>\n",
       "      <td>-0.000436</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>-0.000243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.002914</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.332172</td>\n",
       "      <td>0.818300</td>\n",
       "      <td>0.710593</td>\n",
       "      <td>0.002889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <td>-0.003674</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.018884</td>\n",
       "      <td>-0.000612</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.006359</td>\n",
       "      <td>-0.000480</td>\n",
       "      <td>-0.000529</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-0.000205</td>\n",
       "      <td>0.332172</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.229158</td>\n",
       "      <td>0.213191</td>\n",
       "      <td>0.003646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <td>-0.000856</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.032833</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.011665</td>\n",
       "      <td>0.010660</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>-0.000510</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>-0.000292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010993</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>-0.000225</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>0.818300</td>\n",
       "      <td>0.229158</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968167</td>\n",
       "      <td>0.003258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.033597</td>\n",
       "      <td>0.006590</td>\n",
       "      <td>0.010802</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>-0.000372</td>\n",
       "      <td>-0.000601</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>-0.000308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009088</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.000238</td>\n",
       "      <td>0.002905</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.710593</td>\n",
       "      <td>0.213191</td>\n",
       "      <td>0.968167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var38</th>\n",
       "      <td>-0.005687</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.006497</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.012546</td>\n",
       "      <td>0.013271</td>\n",
       "      <td>0.016628</td>\n",
       "      <td>0.015584</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001412</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370 rows × 370 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ID      var3     var15  imp_ent_var16_ult1  \\\n",
       "ID                       1.000000 -0.001025 -0.002135           -0.003111   \n",
       "var3                    -0.001025  1.000000 -0.004452            0.001868   \n",
       "var15                   -0.002135 -0.004452  1.000000            0.043722   \n",
       "imp_ent_var16_ult1      -0.003111  0.001868  0.043722            1.000000   \n",
       "imp_op_var39_comer_ult1 -0.001436  0.005989  0.094762            0.041221   \n",
       "...                           ...       ...       ...                 ...   \n",
       "saldo_medio_var44_hace2 -0.003772  0.000617  0.029180            0.002655   \n",
       "saldo_medio_var44_hace3 -0.003674  0.000508  0.018884           -0.000612   \n",
       "saldo_medio_var44_ult1  -0.000856  0.000738  0.032833            0.005055   \n",
       "saldo_medio_var44_ult3   0.000297  0.000778  0.033597            0.006590   \n",
       "var38                   -0.005687  0.000071  0.006497            0.000007   \n",
       "\n",
       "                         imp_op_var39_comer_ult1  imp_op_var39_comer_ult3  \\\n",
       "ID                                     -0.001436                -0.004131   \n",
       "var3                                    0.005989                 0.006817   \n",
       "var15                                   0.094762                 0.101177   \n",
       "imp_ent_var16_ult1                      0.041221                 0.034879   \n",
       "imp_op_var39_comer_ult1                 1.000000                 0.886476   \n",
       "...                                          ...                      ...   \n",
       "saldo_medio_var44_hace2                 0.009445                 0.008539   \n",
       "saldo_medio_var44_hace3                 0.005532                 0.006359   \n",
       "saldo_medio_var44_ult1                  0.011665                 0.010660   \n",
       "saldo_medio_var44_ult3                  0.010802                 0.009822   \n",
       "var38                                   0.012546                 0.013271   \n",
       "\n",
       "                         imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
       "ID                                     -0.007277                -0.006302   \n",
       "var3                                    0.001518                 0.001690   \n",
       "var15                                   0.042754                 0.048512   \n",
       "imp_ent_var16_ult1                      0.009896                 0.009377   \n",
       "imp_op_var39_comer_ult1                 0.342709                 0.295295   \n",
       "...                                          ...                      ...   \n",
       "saldo_medio_var44_hace2                -0.000436                -0.000561   \n",
       "saldo_medio_var44_hace3                -0.000480                -0.000529   \n",
       "saldo_medio_var44_ult1                 -0.000254                -0.000510   \n",
       "saldo_medio_var44_ult3                 -0.000372                -0.000601   \n",
       "var38                                   0.016628                 0.015584   \n",
       "\n",
       "                         imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  \\\n",
       "ID                                     -0.006700                -0.006698   \n",
       "var3                                    0.000530                 0.000611   \n",
       "var15                                   0.008805                 0.009678   \n",
       "imp_ent_var16_ult1                      0.000592                 0.002510   \n",
       "imp_op_var39_comer_ult1                 0.032280                 0.054809   \n",
       "...                                          ...                      ...   \n",
       "saldo_medio_var44_hace2                -0.000211                -0.000243   \n",
       "saldo_medio_var44_hace3                -0.000170                -0.000195   \n",
       "saldo_medio_var44_ult1                 -0.000254                -0.000292   \n",
       "saldo_medio_var44_ult3                 -0.000268                -0.000308   \n",
       "var38                                  -0.000315                 0.000682   \n",
       "\n",
       "                         ...  saldo_medio_var29_ult3  saldo_medio_var33_hace2  \\\n",
       "ID                       ...               -0.007631                 0.001986   \n",
       "var3                     ...                0.000229                 0.000716   \n",
       "var15                    ...                0.011623                 0.029358   \n",
       "imp_ent_var16_ult1       ...                0.007428                -0.000864   \n",
       "imp_op_var39_comer_ult1  ...                0.001123                 0.016422   \n",
       "...                      ...                     ...                      ...   \n",
       "saldo_medio_var44_hace2  ...                0.003708                 0.001174   \n",
       "saldo_medio_var44_hace3  ...               -0.000074                -0.000219   \n",
       "saldo_medio_var44_ult1   ...                0.010993                 0.000819   \n",
       "saldo_medio_var44_ult3   ...                0.009088                 0.000977   \n",
       "var38                    ...               -0.001412                 0.004448   \n",
       "\n",
       "                         saldo_medio_var33_hace3  saldo_medio_var33_ult1  \\\n",
       "ID                                      0.003771               -0.001521   \n",
       "var3                                    0.000491                0.000638   \n",
       "var15                                   0.017264                0.028504   \n",
       "imp_ent_var16_ult1                     -0.000632               -0.000548   \n",
       "imp_op_var39_comer_ult1                 0.011719                0.012570   \n",
       "...                                          ...                     ...   \n",
       "saldo_medio_var44_hace2                -0.000187                0.003271   \n",
       "saldo_medio_var44_hace3                -0.000151               -0.000196   \n",
       "saldo_medio_var44_ult1                 -0.000225                0.002493   \n",
       "saldo_medio_var44_ult3                 -0.000238                0.002905   \n",
       "var38                                   0.001605                0.004255   \n",
       "\n",
       "                         saldo_medio_var33_ult3  saldo_medio_var44_hace2  \\\n",
       "ID                                    -0.001216                -0.003772   \n",
       "var3                                   0.000669                 0.000617   \n",
       "var15                                  0.029176                 0.029180   \n",
       "imp_ent_var16_ult1                    -0.000540                 0.002655   \n",
       "imp_op_var39_comer_ult1                0.013703                 0.009445   \n",
       "...                                         ...                      ...   \n",
       "saldo_medio_var44_hace2                0.002914                 1.000000   \n",
       "saldo_medio_var44_hace3               -0.000205                 0.332172   \n",
       "saldo_medio_var44_ult1                 0.002205                 0.818300   \n",
       "saldo_medio_var44_ult3                 0.002574                 0.710593   \n",
       "var38                                  0.004290                 0.002889   \n",
       "\n",
       "                         saldo_medio_var44_hace3  saldo_medio_var44_ult1  \\\n",
       "ID                                     -0.003674               -0.000856   \n",
       "var3                                    0.000508                0.000738   \n",
       "var15                                   0.018884                0.032833   \n",
       "imp_ent_var16_ult1                     -0.000612                0.005055   \n",
       "imp_op_var39_comer_ult1                 0.005532                0.011665   \n",
       "...                                          ...                     ...   \n",
       "saldo_medio_var44_hace2                 0.332172                0.818300   \n",
       "saldo_medio_var44_hace3                 1.000000                0.229158   \n",
       "saldo_medio_var44_ult1                  0.229158                1.000000   \n",
       "saldo_medio_var44_ult3                  0.213191                0.968167   \n",
       "var38                                   0.003646                0.003258   \n",
       "\n",
       "                         saldo_medio_var44_ult3     var38  \n",
       "ID                                     0.000297 -0.005687  \n",
       "var3                                   0.000778  0.000071  \n",
       "var15                                  0.033597  0.006497  \n",
       "imp_ent_var16_ult1                     0.006590  0.000007  \n",
       "imp_op_var39_comer_ult1                0.010802  0.012546  \n",
       "...                                         ...       ...  \n",
       "saldo_medio_var44_hace2                0.710593  0.002889  \n",
       "saldo_medio_var44_hace3                0.213191  0.003646  \n",
       "saldo_medio_var44_ult1                 0.968167  0.003258  \n",
       "saldo_medio_var44_ult3                 1.000000  0.003037  \n",
       "var38                                  0.003037  1.000000  \n",
       "\n",
       "[370 rows x 370 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = X_train.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var29_ult3</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001025</td>\n",
       "      <td>-0.002135</td>\n",
       "      <td>-0.003111</td>\n",
       "      <td>-0.001436</td>\n",
       "      <td>-0.004131</td>\n",
       "      <td>-0.007277</td>\n",
       "      <td>-0.006302</td>\n",
       "      <td>-0.006700</td>\n",
       "      <td>-0.006698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007631</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.003771</td>\n",
       "      <td>-0.001521</td>\n",
       "      <td>-0.001216</td>\n",
       "      <td>-0.003772</td>\n",
       "      <td>-0.003674</td>\n",
       "      <td>-0.000856</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>-0.005687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.004452</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043722</td>\n",
       "      <td>0.094762</td>\n",
       "      <td>0.101177</td>\n",
       "      <td>0.042754</td>\n",
       "      <td>0.048512</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.009678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011623</td>\n",
       "      <td>0.029358</td>\n",
       "      <td>0.017264</td>\n",
       "      <td>0.028504</td>\n",
       "      <td>0.029176</td>\n",
       "      <td>0.029180</td>\n",
       "      <td>0.018884</td>\n",
       "      <td>0.032833</td>\n",
       "      <td>0.033597</td>\n",
       "      <td>0.006497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>0.034879</td>\n",
       "      <td>0.009896</td>\n",
       "      <td>0.009377</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.002510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>-0.000632</td>\n",
       "      <td>-0.000548</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>0.002655</td>\n",
       "      <td>-0.000612</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.006590</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.886476</td>\n",
       "      <td>0.342709</td>\n",
       "      <td>0.295295</td>\n",
       "      <td>0.032280</td>\n",
       "      <td>0.054809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.016422</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.012570</td>\n",
       "      <td>0.013703</td>\n",
       "      <td>0.009445</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.011665</td>\n",
       "      <td>0.010802</td>\n",
       "      <td>0.012546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.332172</td>\n",
       "      <td>0.818300</td>\n",
       "      <td>0.710593</td>\n",
       "      <td>0.002889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.229158</td>\n",
       "      <td>0.213191</td>\n",
       "      <td>0.003646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.968167</td>\n",
       "      <td>0.003258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var38</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370 rows × 370 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID      var3     var15  imp_ent_var16_ult1  \\\n",
       "ID                      NaN -0.001025 -0.002135           -0.003111   \n",
       "var3                    NaN       NaN -0.004452            0.001868   \n",
       "var15                   NaN       NaN       NaN            0.043722   \n",
       "imp_ent_var16_ult1      NaN       NaN       NaN                 NaN   \n",
       "imp_op_var39_comer_ult1 NaN       NaN       NaN                 NaN   \n",
       "...                      ..       ...       ...                 ...   \n",
       "saldo_medio_var44_hace2 NaN       NaN       NaN                 NaN   \n",
       "saldo_medio_var44_hace3 NaN       NaN       NaN                 NaN   \n",
       "saldo_medio_var44_ult1  NaN       NaN       NaN                 NaN   \n",
       "saldo_medio_var44_ult3  NaN       NaN       NaN                 NaN   \n",
       "var38                   NaN       NaN       NaN                 NaN   \n",
       "\n",
       "                         imp_op_var39_comer_ult1  imp_op_var39_comer_ult3  \\\n",
       "ID                                     -0.001436                -0.004131   \n",
       "var3                                    0.005989                 0.006817   \n",
       "var15                                   0.094762                 0.101177   \n",
       "imp_ent_var16_ult1                      0.041221                 0.034879   \n",
       "imp_op_var39_comer_ult1                      NaN                 0.886476   \n",
       "...                                          ...                      ...   \n",
       "saldo_medio_var44_hace2                      NaN                      NaN   \n",
       "saldo_medio_var44_hace3                      NaN                      NaN   \n",
       "saldo_medio_var44_ult1                       NaN                      NaN   \n",
       "saldo_medio_var44_ult3                       NaN                      NaN   \n",
       "var38                                        NaN                      NaN   \n",
       "\n",
       "                         imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
       "ID                                     -0.007277                -0.006302   \n",
       "var3                                    0.001518                 0.001690   \n",
       "var15                                   0.042754                 0.048512   \n",
       "imp_ent_var16_ult1                      0.009896                 0.009377   \n",
       "imp_op_var39_comer_ult1                 0.342709                 0.295295   \n",
       "...                                          ...                      ...   \n",
       "saldo_medio_var44_hace2                      NaN                      NaN   \n",
       "saldo_medio_var44_hace3                      NaN                      NaN   \n",
       "saldo_medio_var44_ult1                       NaN                      NaN   \n",
       "saldo_medio_var44_ult3                       NaN                      NaN   \n",
       "var38                                        NaN                      NaN   \n",
       "\n",
       "                         imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  \\\n",
       "ID                                     -0.006700                -0.006698   \n",
       "var3                                    0.000530                 0.000611   \n",
       "var15                                   0.008805                 0.009678   \n",
       "imp_ent_var16_ult1                      0.000592                 0.002510   \n",
       "imp_op_var39_comer_ult1                 0.032280                 0.054809   \n",
       "...                                          ...                      ...   \n",
       "saldo_medio_var44_hace2                      NaN                      NaN   \n",
       "saldo_medio_var44_hace3                      NaN                      NaN   \n",
       "saldo_medio_var44_ult1                       NaN                      NaN   \n",
       "saldo_medio_var44_ult3                       NaN                      NaN   \n",
       "var38                                        NaN                      NaN   \n",
       "\n",
       "                         ...  saldo_medio_var29_ult3  saldo_medio_var33_hace2  \\\n",
       "ID                       ...               -0.007631                 0.001986   \n",
       "var3                     ...                0.000229                 0.000716   \n",
       "var15                    ...                0.011623                 0.029358   \n",
       "imp_ent_var16_ult1       ...                0.007428                -0.000864   \n",
       "imp_op_var39_comer_ult1  ...                0.001123                 0.016422   \n",
       "...                      ...                     ...                      ...   \n",
       "saldo_medio_var44_hace2  ...                     NaN                      NaN   \n",
       "saldo_medio_var44_hace3  ...                     NaN                      NaN   \n",
       "saldo_medio_var44_ult1   ...                     NaN                      NaN   \n",
       "saldo_medio_var44_ult3   ...                     NaN                      NaN   \n",
       "var38                    ...                     NaN                      NaN   \n",
       "\n",
       "                         saldo_medio_var33_hace3  saldo_medio_var33_ult1  \\\n",
       "ID                                      0.003771               -0.001521   \n",
       "var3                                    0.000491                0.000638   \n",
       "var15                                   0.017264                0.028504   \n",
       "imp_ent_var16_ult1                     -0.000632               -0.000548   \n",
       "imp_op_var39_comer_ult1                 0.011719                0.012570   \n",
       "...                                          ...                     ...   \n",
       "saldo_medio_var44_hace2                      NaN                     NaN   \n",
       "saldo_medio_var44_hace3                      NaN                     NaN   \n",
       "saldo_medio_var44_ult1                       NaN                     NaN   \n",
       "saldo_medio_var44_ult3                       NaN                     NaN   \n",
       "var38                                        NaN                     NaN   \n",
       "\n",
       "                         saldo_medio_var33_ult3  saldo_medio_var44_hace2  \\\n",
       "ID                                    -0.001216                -0.003772   \n",
       "var3                                   0.000669                 0.000617   \n",
       "var15                                  0.029176                 0.029180   \n",
       "imp_ent_var16_ult1                    -0.000540                 0.002655   \n",
       "imp_op_var39_comer_ult1                0.013703                 0.009445   \n",
       "...                                         ...                      ...   \n",
       "saldo_medio_var44_hace2                     NaN                      NaN   \n",
       "saldo_medio_var44_hace3                     NaN                      NaN   \n",
       "saldo_medio_var44_ult1                      NaN                      NaN   \n",
       "saldo_medio_var44_ult3                      NaN                      NaN   \n",
       "var38                                       NaN                      NaN   \n",
       "\n",
       "                         saldo_medio_var44_hace3  saldo_medio_var44_ult1  \\\n",
       "ID                                     -0.003674               -0.000856   \n",
       "var3                                    0.000508                0.000738   \n",
       "var15                                   0.018884                0.032833   \n",
       "imp_ent_var16_ult1                     -0.000612                0.005055   \n",
       "imp_op_var39_comer_ult1                 0.005532                0.011665   \n",
       "...                                          ...                     ...   \n",
       "saldo_medio_var44_hace2                 0.332172                0.818300   \n",
       "saldo_medio_var44_hace3                      NaN                0.229158   \n",
       "saldo_medio_var44_ult1                       NaN                     NaN   \n",
       "saldo_medio_var44_ult3                       NaN                     NaN   \n",
       "var38                                        NaN                     NaN   \n",
       "\n",
       "                         saldo_medio_var44_ult3     var38  \n",
       "ID                                     0.000297 -0.005687  \n",
       "var3                                   0.000778  0.000071  \n",
       "var15                                  0.033597  0.006497  \n",
       "imp_ent_var16_ult1                     0.006590  0.000007  \n",
       "imp_op_var39_comer_ult1                0.010802  0.012546  \n",
       "...                                         ...       ...  \n",
       "saldo_medio_var44_hace2                0.710593  0.002889  \n",
       "saldo_medio_var44_hace3                0.213191  0.003646  \n",
       "saldo_medio_var44_ult1                 0.968167  0.003258  \n",
       "saldo_medio_var44_ult3                      NaN  0.003037  \n",
       "var38                                       NaN       NaN  \n",
       "\n",
       "[370 rows x 370 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imp_op_var41_comer_ult1', 'imp_op_var41_comer_ult3', 'imp_op_var39_efect_ult1', 'imp_op_var39_efect_ult3', 'imp_op_var39_ult1', 'ind_var8', 'ind_var13_corto_0', 'ind_var13_corto', 'ind_var13_largo', 'ind_var13_medio', 'ind_var13', 'ind_var18', 'ind_var24', 'ind_var26_0', 'ind_var26_cte', 'ind_var26', 'ind_var25_0', 'ind_var25', 'ind_var29_0', 'ind_var29', 'ind_var31', 'ind_var32_0', 'ind_var32', 'ind_var33', 'ind_var34', 'ind_var37_0', 'ind_var37', 'ind_var40_0', 'ind_var40', 'ind_var41_0', 'ind_var39', 'ind_var44', 'num_var1_0', 'num_var1', 'num_var5_0', 'num_var5', 'num_var6_0', 'num_var6', 'num_var8_0', 'num_var8', 'num_var12', 'num_var13_0', 'num_var13_corto_0', 'num_var13_corto', 'num_var13_largo_0', 'num_var13_largo', 'num_var13_medio_0', 'num_var13_medio', 'num_var13', 'num_var14', 'num_var18_0', 'num_var18', 'num_var20_0', 'num_var20', 'num_var24_0', 'num_var24', 'num_var26', 'num_var25_0', 'num_var25', 'num_op_var40_ult3', 'num_op_var41_ult3', 'num_op_var39_hace2', 'num_op_var39_hace3', 'num_op_var39_ult1', 'num_op_var39_ult3', 'num_var29_0', 'num_var29', 'num_var31_0', 'num_var32_0', 'num_var32', 'num_var33_0', 'num_var33', 'num_var34_0', 'num_var34', 'num_var35', 'num_var37_0', 'num_var37', 'num_var40_0', 'num_var40', 'num_var41_0', 'num_var39', 'num_var42_0', 'num_var44_0', 'num_var44', 'saldo_var6', 'saldo_var18', 'saldo_var24', 'saldo_var25', 'saldo_var29', 'saldo_var31', 'saldo_var34', 'saldo_var42', 'delta_imp_amort_var18_1y3', 'delta_imp_amort_var34_1y3', 'delta_num_aport_var13_1y3', 'delta_num_aport_var17_1y3', 'delta_num_aport_var33_1y3', 'delta_num_compra_var44_1y3', 'delta_num_reemb_var13_1y3', 'delta_num_reemb_var17_1y3', 'delta_num_reemb_var33_1y3', 'delta_num_trasp_var17_in_1y3', 'delta_num_trasp_var17_out_1y3', 'delta_num_trasp_var33_in_1y3', 'delta_num_trasp_var33_out_1y3', 'delta_num_venta_var44_1y3', 'imp_amort_var18_ult1', 'imp_amort_var34_ult1', 'imp_aport_var17_hace3', 'imp_reemb_var33_ult1', 'imp_trasp_var33_out_ult1', 'imp_venta_var44_ult1', 'ind_var10cte_ult1', 'ind_var9_cte_ult1', 'ind_var9_ult1', 'num_var7_emit_ult1', 'num_med_var22_ult3', 'num_meses_var5_ult3', 'num_meses_var8_ult3', 'num_meses_var12_ult3', 'num_meses_var13_corto_ult3', 'num_meses_var13_medio_ult3', 'num_meses_var17_ult3', 'num_meses_var33_ult3', 'num_meses_var44_ult3', 'num_op_var39_comer_ult1', 'num_op_var39_comer_ult3', 'num_op_var40_comer_ult3', 'num_op_var41_comer_ult1', 'num_op_var41_comer_ult3', 'num_op_var41_efect_ult3', 'num_op_var39_efect_ult1', 'num_op_var39_efect_ult3', 'num_reemb_var13_ult1', 'num_reemb_var17_hace3', 'num_reemb_var33_ult1', 'num_trasp_var17_in_hace3', 'num_trasp_var17_in_ult1', 'num_trasp_var17_out_ult1', 'num_trasp_var33_in_hace3', 'num_trasp_var33_in_ult1', 'num_trasp_var33_out_ult1', 'num_venta_var44_hace3', 'num_var45_hace2', 'num_var45_ult3', 'saldo_medio_var8_ult1', 'saldo_medio_var8_ult3', 'saldo_medio_var12_ult1', 'saldo_medio_var12_ult3', 'saldo_medio_var13_corto_ult1', 'saldo_medio_var13_corto_ult3', 'saldo_medio_var13_largo_ult1', 'saldo_medio_var13_largo_ult3', 'saldo_medio_var13_medio_hace2', 'saldo_medio_var13_medio_ult1', 'saldo_medio_var13_medio_ult3', 'saldo_medio_var17_hace2', 'saldo_medio_var17_hace3', 'saldo_medio_var17_ult1', 'saldo_medio_var17_ult3', 'saldo_medio_var29_ult1', 'saldo_medio_var29_ult3', 'saldo_medio_var33_ult1', 'saldo_medio_var33_ult3', 'saldo_medio_var44_ult1', 'saldo_medio_var44_ult3']\n"
     ]
    }
   ],
   "source": [
    "# Find index of feature columns with correlation greater than 0.9\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'var3', 'var15', 'imp_ent_var16_ult1', 'imp_op_var39_comer_ult1',\n",
       "       'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1',\n",
       "       'imp_op_var40_comer_ult3', 'imp_op_var40_efect_ult1',\n",
       "       'imp_op_var40_efect_ult3',\n",
       "       ...\n",
       "       'saldo_medio_var13_largo_hace2', 'saldo_medio_var13_largo_hace3',\n",
       "       'saldo_medio_var13_medio_hace3', 'saldo_medio_var29_hace2',\n",
       "       'saldo_medio_var29_hace3', 'saldo_medio_var33_hace2',\n",
       "       'saldo_medio_var33_hace3', 'saldo_medio_var44_hace2',\n",
       "       'saldo_medio_var44_hace3', 'var38'],\n",
       "      dtype='object', length=204)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop Marked Features\n",
    "df1 = X_train.drop(columns=to_drop, axis=1)\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var13_largo_hace2</th>\n",
       "      <th>saldo_medio_var13_largo_hace3</th>\n",
       "      <th>saldo_medio_var13_medio_hace3</th>\n",
       "      <th>saldo_medio_var29_hace2</th>\n",
       "      <th>saldo_medio_var29_hace3</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>var38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39205.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49278.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67333.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64007.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117310.979016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76015</th>\n",
       "      <td>151829</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60926.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76016</th>\n",
       "      <td>151830</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>118634.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76017</th>\n",
       "      <td>151835</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74028.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76018</th>\n",
       "      <td>151836</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84278.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76019</th>\n",
       "      <td>151838</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117310.979016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76020 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  var3  var15  imp_ent_var16_ult1  imp_op_var39_comer_ult1  \\\n",
       "0           1     2     23                 0.0                      0.0   \n",
       "1           3     2     34                 0.0                      0.0   \n",
       "2           4     2     23                 0.0                      0.0   \n",
       "3           8     2     37                 0.0                    195.0   \n",
       "4          10     2     39                 0.0                      0.0   \n",
       "...       ...   ...    ...                 ...                      ...   \n",
       "76015  151829     2     48                 0.0                      0.0   \n",
       "76016  151830     2     39                 0.0                      0.0   \n",
       "76017  151835     2     23                 0.0                      0.0   \n",
       "76018  151836     2     25                 0.0                      0.0   \n",
       "76019  151838     2     46                 0.0                      0.0   \n",
       "\n",
       "       imp_op_var39_comer_ult3  imp_op_var40_comer_ult1  \\\n",
       "0                          0.0                      0.0   \n",
       "1                          0.0                      0.0   \n",
       "2                          0.0                      0.0   \n",
       "3                        195.0                      0.0   \n",
       "4                          0.0                      0.0   \n",
       "...                        ...                      ...   \n",
       "76015                      0.0                      0.0   \n",
       "76016                      0.0                      0.0   \n",
       "76017                      0.0                      0.0   \n",
       "76018                      0.0                      0.0   \n",
       "76019                      0.0                      0.0   \n",
       "\n",
       "       imp_op_var40_comer_ult3  imp_op_var40_efect_ult1  \\\n",
       "0                          0.0                      0.0   \n",
       "1                          0.0                      0.0   \n",
       "2                          0.0                      0.0   \n",
       "3                          0.0                      0.0   \n",
       "4                          0.0                      0.0   \n",
       "...                        ...                      ...   \n",
       "76015                      0.0                      0.0   \n",
       "76016                      0.0                      0.0   \n",
       "76017                      0.0                      0.0   \n",
       "76018                      0.0                      0.0   \n",
       "76019                      0.0                      0.0   \n",
       "\n",
       "       imp_op_var40_efect_ult3  ...  saldo_medio_var13_largo_hace2  \\\n",
       "0                          0.0  ...                            0.0   \n",
       "1                          0.0  ...                            0.0   \n",
       "2                          0.0  ...                            0.0   \n",
       "3                          0.0  ...                            0.0   \n",
       "4                          0.0  ...                            0.0   \n",
       "...                        ...  ...                            ...   \n",
       "76015                      0.0  ...                            0.0   \n",
       "76016                      0.0  ...                            0.0   \n",
       "76017                      0.0  ...                            0.0   \n",
       "76018                      0.0  ...                            0.0   \n",
       "76019                      0.0  ...                            0.0   \n",
       "\n",
       "       saldo_medio_var13_largo_hace3  saldo_medio_var13_medio_hace3  \\\n",
       "0                                0.0                              0   \n",
       "1                                0.0                              0   \n",
       "2                                0.0                              0   \n",
       "3                                0.0                              0   \n",
       "4                                0.0                              0   \n",
       "...                              ...                            ...   \n",
       "76015                            0.0                              0   \n",
       "76016                            0.0                              0   \n",
       "76017                            0.0                              0   \n",
       "76018                            0.0                              0   \n",
       "76019                            0.0                              0   \n",
       "\n",
       "       saldo_medio_var29_hace2  saldo_medio_var29_hace3  \\\n",
       "0                          0.0                      0.0   \n",
       "1                          0.0                      0.0   \n",
       "2                          0.0                      0.0   \n",
       "3                          0.0                      0.0   \n",
       "4                          0.0                      0.0   \n",
       "...                        ...                      ...   \n",
       "76015                      0.0                      0.0   \n",
       "76016                      0.0                      0.0   \n",
       "76017                      0.0                      0.0   \n",
       "76018                      0.0                      0.0   \n",
       "76019                      0.0                      0.0   \n",
       "\n",
       "       saldo_medio_var33_hace2  saldo_medio_var33_hace3  \\\n",
       "0                          0.0                      0.0   \n",
       "1                          0.0                      0.0   \n",
       "2                          0.0                      0.0   \n",
       "3                          0.0                      0.0   \n",
       "4                          0.0                      0.0   \n",
       "...                        ...                      ...   \n",
       "76015                      0.0                      0.0   \n",
       "76016                      0.0                      0.0   \n",
       "76017                      0.0                      0.0   \n",
       "76018                      0.0                      0.0   \n",
       "76019                      0.0                      0.0   \n",
       "\n",
       "       saldo_medio_var44_hace2  saldo_medio_var44_hace3          var38  \n",
       "0                          0.0                      0.0   39205.170000  \n",
       "1                          0.0                      0.0   49278.030000  \n",
       "2                          0.0                      0.0   67333.770000  \n",
       "3                          0.0                      0.0   64007.970000  \n",
       "4                          0.0                      0.0  117310.979016  \n",
       "...                        ...                      ...            ...  \n",
       "76015                      0.0                      0.0   60926.490000  \n",
       "76016                      0.0                      0.0  118634.520000  \n",
       "76017                      0.0                      0.0   74028.150000  \n",
       "76018                      0.0                      0.0   84278.160000  \n",
       "76019                      0.0                      0.0  117310.979016  \n",
       "\n",
       "[76020 rows x 204 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing Kbest and corrolation matrix features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the two best features\n",
    "sel_kbest = SelectKBest(mutual_info_classif, k=16)\n",
    "X_new = sel_kbest.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 204)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now select features based on top 10 percentile\n",
    "sel_percent = SelectPercentile(f_classif, percentile= 55.1)\n",
    "X_percent = sel_percent.fit_transform(X_train, y_train)\n",
    "X_percent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df1.columns.to_numpy()\n",
    "b = X_train.columns[sel_kbest.get_support()].to_numpy()\n",
    "c = X_train.columns[sel_percent.get_support()].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrolation features 204\n",
      "Kbest features 15\n",
      "Common features 9\n"
     ]
    }
   ],
   "source": [
    "common_features = np.intersect1d(a, b)\n",
    "print(f'Corrolation features {len(a)}')\n",
    "print(f'Kbest features {len(b)}')\n",
    "print(f'Common features {len(common_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrolation features 204\n",
      "Kbest features 204\n",
      "Common features 111\n"
     ]
    }
   ],
   "source": [
    "common_features = np.intersect1d(a, c)\n",
    "print(f'Corrolation features {len(a)}')\n",
    "print(f'Kbest features {len(c)}')\n",
    "print(f'Common features {len(common_features)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that we have dropped the third column from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.17.2-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 818 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.24.2 in /Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages (from mlxtend) (1.0.3)\n",
      "Requirement already satisfied: setuptools in /Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages (from mlxtend) (46.1.3.post20200330)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages (from mlxtend) (1.18.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in /Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages (from mlxtend) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages (from mlxtend) (0.14.1)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages (from mlxtend) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages (from mlxtend) (3.1.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages (from pandas>=0.24.2->mlxtend) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages (from pandas>=0.24.2->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.6)\n",
      "Requirement already satisfied: six>=1.5 in /Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.24.2->mlxtend) (1.14.0)\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Wrapper Methods** <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from the subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "\n",
    "- Some common examples of wrapper methods are \n",
    "\n",
    "  - 1. Forward selection, \n",
    "  - 2. Backward elimination, \n",
    "  - 3. Exhaustive feature selection,\n",
    "  - 4. Recursive feature elimination.\n",
    "  - 5. Recursive feature elimination with cross-validation\n",
    "  \n",
    "  \n",
    "- Wrapper methods can be explained with the help of following graphic:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Wrapper Methods](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537549832/Image2_ajaeo8.png)\n",
    "\n",
    "\n",
    "\n",
    "### Image source : AnalyticsVidhya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.1 Sequential Feature Selection** <a class=\"anchor\" id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1.1 Sequential Forward Selection (SFS)** <a class=\"anchor\" id=\"2.1.1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1) \n",
    "\n",
    "\n",
    "- Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "\n",
    "- The procedure starts with an empty set of features [reduced set]. The best of the original features is determined and added to the reduced set. At each subsequent iteration, the best of the remaining original attributes is added to the set.\n",
    "\n",
    "- Step forward feature selection starts by evaluating all features individually and selects the one that generates the best performing algorithm, according to a pre-set evaluation criteria. In the second step, it evaluates all possible combinations of the selected feature and a second feature, and selects the pair that produce the best performing algorithm based on the same pre-set criteria.\n",
    "\n",
    "- The pre-set criteria can be the roc_auc for classification and the r squared for regression for example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step forward feature selection\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_trian, X_test = load_santander()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removed correlated  features\n",
    "# X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "# X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.iloc[:10000, :20]\n",
    "y_train = y_train[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "exception calling callback for <Future at 0x10daa6850 state=finished raised TerminatedWorkerError>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/externals/loky/_base.py\", line 625, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"/Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/parallel.py\", line 340, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"/Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/parallel.py\", line 769, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"/Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/parallel.py\", line 835, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/parallel.py\", line 754, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 551, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "  File \"/Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/externals/loky/reusable_executor.py\", line 160, in submit\n",
      "    fn, *args, **kwargs)\n",
      "  File \"/Users/celal/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 1027, in submit\n",
      "    raise self._flags.broken\n",
      "joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6)}\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-99857e6e83bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m            cv=3)\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msfs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msfs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/mlxtend/feature_selection/sequential_feature_selector.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, custom_feature_names, groups, **fit_params)\u001b[0m\n\u001b[1;32m    436\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                         \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                         \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m                     )\n\u001b[1;32m    440\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/mlxtend/feature_selection/sequential_feature_selector.py\u001b[0m in \u001b[0;36m_inclusion\u001b[0;34m(self, orig_set, subset, X, y, ignore_feature, groups, **fit_params)\u001b[0m\n\u001b[1;32m    605\u001b[0m                              \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                              groups=groups, **fit_params)\n\u001b[0;32m--> 607\u001b[0;31m                             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m                             if feature != ignore_feature)\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/externals/loky/_base.py\u001b[0m in \u001b[0;36m_invoke_callbacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exception calling callback for %r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, out)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \"\"\"\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSafeFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_future_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/externals/loky/reusable_executor.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_submit_resize_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             return super(_ReusablePoolExecutor, self).submit(\n\u001b[0;32m--> 160\u001b[0;31m                 fn, *args, **kwargs)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phd_analysis/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m                 raise ShutdownExecutorError(\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6)}"
     ]
    }
   ],
   "source": [
    "# step forward feature selection\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "sfs1 = SFS(RandomForestRegressor(), \n",
    "           k_features=12, \n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           verbose=1,\n",
    "           scoring='accuracy', # r2\n",
    "           n_jobs=-1,\n",
    "           cv=3)\n",
    "\n",
    "sfs1 = sfs1.fit(np.array(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, 19, 21, 23, 25, 27, 30, 32, 34, 36, 39)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs1.k_feature_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_idx</th>\n",
       "      <th>cv_scores</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>feature_names</th>\n",
       "      <th>ci_bound</th>\n",
       "      <th>std_dev</th>\n",
       "      <th>std_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(25,)</td>\n",
       "      <td>[0.013905972527015753, 0.013718479889091584, 0...</td>\n",
       "      <td>0.0119989</td>\n",
       "      <td>(25,)</td>\n",
       "      <td>0.00577332</td>\n",
       "      <td>0.00256555</td>\n",
       "      <td>0.00181411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(25, 30)</td>\n",
       "      <td>[0.017762936714548205, 0.01965278216441102, 0....</td>\n",
       "      <td>0.0158719</td>\n",
       "      <td>(25, 30)</td>\n",
       "      <td>0.00919071</td>\n",
       "      <td>0.00408416</td>\n",
       "      <td>0.00288794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(25, 30, 32)</td>\n",
       "      <td>[0.020253516357089385, 0.022870980867819912, 0...</td>\n",
       "      <td>0.0184375</td>\n",
       "      <td>(25, 30, 32)</td>\n",
       "      <td>0.0102311</td>\n",
       "      <td>0.00454648</td>\n",
       "      <td>0.00321484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1, 25, 30, 32)</td>\n",
       "      <td>[0.02023550805269614, 0.024006812376604625, 0....</td>\n",
       "      <td>0.0185314</td>\n",
       "      <td>(1, 25, 30, 32)</td>\n",
       "      <td>0.0119382</td>\n",
       "      <td>0.00530509</td>\n",
       "      <td>0.00375126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(1, 21, 25, 30, 32)</td>\n",
       "      <td>[0.02166100409435545, 0.023753075876580265, 0....</td>\n",
       "      <td>0.0220395</td>\n",
       "      <td>(1, 21, 25, 30, 32)</td>\n",
       "      <td>0.0028649</td>\n",
       "      <td>0.0012731</td>\n",
       "      <td>0.000900219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(1, 21, 25, 30, 32, 34)</td>\n",
       "      <td>[0.021905495713307133, 0.023780366571690803, 0...</td>\n",
       "      <td>0.0221611</td>\n",
       "      <td>(1, 21, 25, 30, 32, 34)</td>\n",
       "      <td>0.00277053</td>\n",
       "      <td>0.00123117</td>\n",
       "      <td>0.000870565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(1, 21, 25, 30, 32, 34, 39)</td>\n",
       "      <td>[0.021509096672572947, 0.02355552508316905, 0....</td>\n",
       "      <td>0.0217535</td>\n",
       "      <td>(1, 21, 25, 30, 32, 34, 39)</td>\n",
       "      <td>0.00311095</td>\n",
       "      <td>0.00138244</td>\n",
       "      <td>0.000977535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(1, 19, 21, 25, 30, 32, 34, 39)</td>\n",
       "      <td>[0.022089576553301793, 0.023697978089996186, 0...</td>\n",
       "      <td>0.0222841</td>\n",
       "      <td>(1, 19, 21, 25, 30, 32, 34, 39)</td>\n",
       "      <td>0.00243888</td>\n",
       "      <td>0.00108379</td>\n",
       "      <td>0.000766355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(1, 19, 21, 23, 25, 30, 32, 34, 39)</td>\n",
       "      <td>[0.021837858336232085, 0.023807091421317317, 0...</td>\n",
       "      <td>0.021896</td>\n",
       "      <td>(1, 19, 21, 23, 25, 30, 32, 34, 39)</td>\n",
       "      <td>0.00345914</td>\n",
       "      <td>0.00153717</td>\n",
       "      <td>0.00108694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(1, 19, 21, 23, 25, 30, 32, 34, 36, 39)</td>\n",
       "      <td>[0.021555885548152243, 0.024098695893436495, 0...</td>\n",
       "      <td>0.021601</td>\n",
       "      <td>(1, 19, 21, 23, 25, 30, 32, 34, 36, 39)</td>\n",
       "      <td>0.00454842</td>\n",
       "      <td>0.00202122</td>\n",
       "      <td>0.00142922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(1, 19, 21, 23, 25, 27, 30, 32, 34, 36, 39)</td>\n",
       "      <td>[0.021548473362879683, 0.02342104906717124, 0....</td>\n",
       "      <td>0.0216668</td>\n",
       "      <td>(1, 19, 21, 23, 25, 27, 30, 32, 34, 36, 39)</td>\n",
       "      <td>0.00312018</td>\n",
       "      <td>0.00138654</td>\n",
       "      <td>0.000980434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(1, 8, 19, 21, 23, 25, 27, 30, 32, 34, 36, 39)</td>\n",
       "      <td>[0.022905259101268727, 0.024728412494279817, 0...</td>\n",
       "      <td>0.0217365</td>\n",
       "      <td>(1, 8, 19, 21, 23, 25, 27, 30, 32, 34, 36, 39)</td>\n",
       "      <td>0.00682921</td>\n",
       "      <td>0.00303476</td>\n",
       "      <td>0.0021459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       feature_idx  \\\n",
       "1                                            (25,)   \n",
       "2                                         (25, 30)   \n",
       "3                                     (25, 30, 32)   \n",
       "4                                  (1, 25, 30, 32)   \n",
       "5                              (1, 21, 25, 30, 32)   \n",
       "6                          (1, 21, 25, 30, 32, 34)   \n",
       "7                      (1, 21, 25, 30, 32, 34, 39)   \n",
       "8                  (1, 19, 21, 25, 30, 32, 34, 39)   \n",
       "9              (1, 19, 21, 23, 25, 30, 32, 34, 39)   \n",
       "10         (1, 19, 21, 23, 25, 30, 32, 34, 36, 39)   \n",
       "11     (1, 19, 21, 23, 25, 27, 30, 32, 34, 36, 39)   \n",
       "12  (1, 8, 19, 21, 23, 25, 27, 30, 32, 34, 36, 39)   \n",
       "\n",
       "                                            cv_scores  avg_score  \\\n",
       "1   [0.013905972527015753, 0.013718479889091584, 0...  0.0119989   \n",
       "2   [0.017762936714548205, 0.01965278216441102, 0....  0.0158719   \n",
       "3   [0.020253516357089385, 0.022870980867819912, 0...  0.0184375   \n",
       "4   [0.02023550805269614, 0.024006812376604625, 0....  0.0185314   \n",
       "5   [0.02166100409435545, 0.023753075876580265, 0....  0.0220395   \n",
       "6   [0.021905495713307133, 0.023780366571690803, 0...  0.0221611   \n",
       "7   [0.021509096672572947, 0.02355552508316905, 0....  0.0217535   \n",
       "8   [0.022089576553301793, 0.023697978089996186, 0...  0.0222841   \n",
       "9   [0.021837858336232085, 0.023807091421317317, 0...   0.021896   \n",
       "10  [0.021555885548152243, 0.024098695893436495, 0...   0.021601   \n",
       "11  [0.021548473362879683, 0.02342104906717124, 0....  0.0216668   \n",
       "12  [0.022905259101268727, 0.024728412494279817, 0...  0.0217365   \n",
       "\n",
       "                                     feature_names    ci_bound     std_dev  \\\n",
       "1                                            (25,)  0.00577332  0.00256555   \n",
       "2                                         (25, 30)  0.00919071  0.00408416   \n",
       "3                                     (25, 30, 32)   0.0102311  0.00454648   \n",
       "4                                  (1, 25, 30, 32)   0.0119382  0.00530509   \n",
       "5                              (1, 21, 25, 30, 32)   0.0028649   0.0012731   \n",
       "6                          (1, 21, 25, 30, 32, 34)  0.00277053  0.00123117   \n",
       "7                      (1, 21, 25, 30, 32, 34, 39)  0.00311095  0.00138244   \n",
       "8                  (1, 19, 21, 25, 30, 32, 34, 39)  0.00243888  0.00108379   \n",
       "9              (1, 19, 21, 23, 25, 30, 32, 34, 39)  0.00345914  0.00153717   \n",
       "10         (1, 19, 21, 23, 25, 30, 32, 34, 36, 39)  0.00454842  0.00202122   \n",
       "11     (1, 19, 21, 23, 25, 27, 30, 32, 34, 36, 39)  0.00312018  0.00138654   \n",
       "12  (1, 8, 19, 21, 23, 25, 27, 30, 32, 34, 36, 39)  0.00682921  0.00303476   \n",
       "\n",
       "        std_err  \n",
       "1    0.00181411  \n",
       "2    0.00288794  \n",
       "3    0.00321484  \n",
       "4    0.00375126  \n",
       "5   0.000900219  \n",
       "6   0.000870565  \n",
       "7   0.000977535  \n",
       "8   0.000766355  \n",
       "9    0.00108694  \n",
       "10   0.00142922  \n",
       "11  0.000980434  \n",
       "12    0.0021459  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(sfs1.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plot_sfs(sfs1.get_metric_dict(), kind='std_dev')\n",
    "\n",
    "plt.title('Sequential Forward Selection (w. StdDev)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-38fff345a598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msfs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_feature_idx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "X_train.columns[list(sfs1.k_feature_idx_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that forward feature selection results in the above columns being selected from all the given columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1.2 Sequential Backward Elimination (SBS)** <a class=\"anchor\" id=\"2.1.2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1) \n",
    "\n",
    "\n",
    "- In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n",
    "\n",
    "- The procedure starts with the full set of attributes. At each step, it removes the worst attribute remaining in the set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   13.3s finished\n",
      "\n",
      "[2020-04-29 17:58:32] Features: 19/10 -- score: -0.11590889817655962[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:   11.3s finished\n",
      "\n",
      "[2020-04-29 17:58:43] Features: 18/10 -- score: -0.04251083277553428[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    9.8s finished\n",
      "\n",
      "[2020-04-29 17:58:53] Features: 17/10 -- score: -0.03187688786864359[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:    9.4s finished\n",
      "\n",
      "[2020-04-29 17:59:02] Features: 16/10 -- score: -0.028095426354444324[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    8.4s finished\n",
      "\n",
      "[2020-04-29 17:59:11] Features: 15/10 -- score: -0.029570076726317767[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    8.0s finished\n",
      "\n",
      "[2020-04-29 17:59:19] Features: 14/10 -- score: -0.03047139408647966[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:    7.1s finished\n",
      "\n",
      "[2020-04-29 17:59:26] Features: 13/10 -- score: -0.028491520194414033[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    6.7s finished\n",
      "\n",
      "[2020-04-29 17:59:32] Features: 12/10 -- score: -0.03276610964371544[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    6.2s finished\n",
      "\n",
      "[2020-04-29 17:59:39] Features: 11/10 -- score: -0.03143173728822298[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    5.8s finished\n",
      "\n",
      "[2020-04-29 17:59:44] Features: 10/10 -- score: -0.03037215271988436"
     ]
    }
   ],
   "source": [
    "# step backward feature elimination\n",
    "\n",
    "sfs1 = SFS(RandomForestRegressor(), \n",
    "           k_features=10, \n",
    "           forward=False, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=3)\n",
    "\n",
    "sfs1 = sfs1.fit(np.array(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 4, 5, 10, 11, 13, 14, 16, 17)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs1.k_feature_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var3', 'imp_ent_var16_ult1', 'imp_op_var39_comer_ult1',\n",
       "       'imp_op_var39_comer_ult3', 'imp_op_var40_ult1',\n",
       "       'imp_op_var41_comer_ult1', 'imp_op_var41_efect_ult1',\n",
       "       'imp_op_var41_efect_ult3', 'imp_op_var39_efect_ult1',\n",
       "       'imp_op_var39_efect_ult3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns[list(sfs1.k_feature_idx_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plot_sfs(sfs1.get_metric_dict(), kind='std_dev')\n",
    "\n",
    "plt.title('Sequential Forward Selection (w. StdDev)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, backward feature elimination results in the following columns being selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1.3 Sequential Floating (SFFS and SFBS)** <a class=\"anchor\" id=\"2.1.2\"></a>\n",
    "- For both backward and forward selection, sequential floating is an extension to LRS. Rather than choosing values for L and R to add and remove features, we determine this from the data directly.\n",
    "- During the search, the size of the subset will be floating up and down since we are adding and removing features.\n",
    "- How it works\n",
    "    - Step floating forward selection: After each forward step, SFFS performs backward steps as long as the objective function increases.\n",
    "    - Step floating backward selection: After each backward step, SFBS performs forward steps as long as the objective function increases.\n",
    "    \n",
    "\n",
    "- Here’s a detailed explanation of how SFFS works in practice:\n",
    "    * Start from an empty set.\n",
    "    * Select the best feature as we usually do in SFS and add it.\n",
    "    * Then select the worst feature from this subset.\n",
    "    * Evaluate and check whether the objective function improves or not by deleting this feature. If it’s improving, we delete this feature; otherwise, we keep it.\n",
    "    * Repeat from step 2 until the stop criterion is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1.4 Bidirectional Search (BDS)** <a class=\"anchor\" id=\"2.1.4\"></a>\n",
    "\n",
    "BDS applies SFS and SBS concurrently—SFS is performed from the empty set of features and SBS is performed from the full set of features.\n",
    "But this can lead to an issue of converging to a different solution. To avoid this and to guarantee SFS and SBS converge to the same solution, we make the following constraints:\n",
    "Features already selected by SFS are not removed by SBS.\n",
    "Features already removed by SBS are not added by SFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1.5 Exhaustive Feature Selection** <a class=\"anchor\" id=\"2.1.5\"></a>\n",
    "\n",
    "[Table of Contents](#0.1) \n",
    "\n",
    "\n",
    "- In an exhaustive feature selection the best subset of features is selected, over all possible feature subsets, by optimizing a specified performance metric for a certain machine learning algorithm. For example, if the classifier is a logistic regression and the dataset consists of 4 features, the algorithm will evaluate all 15 feature combinations as follows:\n",
    "\n",
    "  - all possible combinations of 1 feature\n",
    "  - all possible combinations of 2 features\n",
    "  - all possible combinations of 3 features\n",
    "  - all the 4 features\n",
    "  \n",
    "  \n",
    "and select the one that results in the best performance (e.g., classification accuracy) of the logistic regression classifier.\n",
    "\n",
    "- This is another greedy algorithm as it evaluates all possible feature combinations. It is quite computationally expensive, and sometimes, if feature space is big, even unfeasible.\n",
    "\n",
    "- There is a special package for python that implements this type of feature selection: mlxtend.\n",
    "\n",
    "- In the mlxtend implementation of the exhaustive feature selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features.\n",
    "\n",
    "- This is somewhat arbitrary because we may be selecting a subopimal number of features, or likewise, a high number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1.6 Limitations of Step Forward/Backward Selection** <a class=\"anchor\" id=\"2.1.6\"></a>\n",
    "\n",
    "[Table of Contents](#0.1) \n",
    "- Besides the drawbacks of computation costs, there are some other points to pay attention to when working with SFS & SBS:\n",
    "- Since we know that SFS adds features at each iteration, a problem can occur when we add up a feature that was useful in the beginning, but after adding more ones, is now non-useful. At this point, there’s no way to remove this kind of feature.\n",
    "- The same thing happens to SBS but in the reverse direction—this is because of the inability of SBS to see the usefulness of a feature after being removed from the feature set.\n",
    "- For those reasons, and for more generalization for SBS and SFS, there are two methods that can solve such an issue: LRS and sequential floating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1.7 Difference between filter and wrapper methods** <a class=\"anchor\" id=\"2.1.7\"></a>\n",
    "[Table of Contents](#0.1) \n",
    "\n",
    "- Filter methods do not incorporate a machine learning model in order to determine if a feature is good or bad whereas wrapper methods use a machine learning model and train it the feature to decide if it is essential or not.\n",
    "- Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally costly, and in the case of massive datasets, wrapper methods are not the most effective feature selection method to consider.\n",
    "- Filter methods may fail to find the best subset of features in situations when there is not enough data to model the statistical correlation of the features, but wrapper methods can always provide the best subset of features because of their exhaustive nature.\n",
    "- Using features from wrapper methods in your final machine learning model can lead to overfitting as wrapper methods already train machine learning models with the features and it affects the true power of learning. But the features from filter methods will not lead to overfitting in most of the cases\n",
    "- So far you have studied the importance of feature selection, understood its difference with dimensionality reduction. You also covered various types of feature selection methods. So far, so good!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.2 Recursive Feature Selection** <a class=\"anchor\" id=\"2.2\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.1 Recursive Feature elimination** <a class=\"anchor\" id=\"2.2.1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1) \n",
    "\n",
    "\n",
    "- It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.\n",
    "\n",
    "- Recursive feature elimination performs a greedy search to find the best performing feature subset. It iteratively creates models and determines the best or the worst performing feature at each iteration. It constructs the subsequent models with the left features until all the features are explored. It then ranks the features based on the order of their elimination. In the worst case, if a dataset contains N number of features RFE will do a greedy search for 2N combinations of features.\n",
    "\n",
    "- Source : https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html#sphx-glr-auto-examples-feature-selection-plot-rfe-digits-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.2 Recursive Feature Elimination with Cross-Validation** <a class=\"anchor\" id=\"2.2.2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1) \n",
    "\n",
    "\n",
    "- **Recursive Feature Elimination with Cross-Validated (RFECV)** feature selection technique selects the best subset of features for the estimator by removing 0 to N features iteratively using recursive feature elimination.\n",
    "\n",
    "- Then it selects the best subset based on the accuracy or cross-validation score or roc-auc of the model. Recursive feature elimination technique eliminates n features from a model by fitting the model multiple times and at each step, removing the weakest features.\n",
    "\n",
    "- Please see my kernel - [Extensive Analysis - EDA + FE + Modelling : Section 19 Recursive FeaTure Elimination with Cross-Validation](https://www.kaggle.com/prashant111/extensive-analysis-eda-fe-modelling)\n",
    "\n",
    "- Source : https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Embedded Methods** <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Embedded methods are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration. Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold.\n",
    "\n",
    "- This is why Regularization methods are also called penalization methods that introduce additional constraints into the optimization of a predictive algorithm (such as a regression algorithm) that bias the model toward lower complexity (fewer coefficients).\n",
    "\n",
    "- Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt penalization functions to reduce overfitting.\n",
    "\n",
    "- Embedded methods can be explained with the help of following graphic:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Embedded Methods](https://www.analyticsvidhya.com/wp-content/uploads/2016/11/Embedded_1.png)\n",
    "\n",
    "\n",
    "### Image source : AnalyticsVidhya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 ** LASSO Regression<a class=\"anchor\" id=\"3.1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Lasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients.\n",
    "\n",
    "- Regularisation consists in adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model and in other words to avoid overfitting. In linear model regularisation, the penalty is applied over the coefficients that multiply each of the predictors. From the different types of regularisation, Lasso or l1 has the property that is able to shrink some of the coefficients to zero. Therefore, that feature can be removed from the model.\n",
    "\n",
    "- I will demonstrate how to select features using the Lasso regularisation on the House Price dataset from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1460, 81)"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('../../data/boston/train.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1460, 38)"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((1022, 37), (438, 37))"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['SalePrice'], axis=1),\n",
    "    data['SalePrice'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "StandardScaler(copy=True, with_mean=True, with_std=True)"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "# the features in the house dataset are in very\n",
    "# different scales, so it helps the regression to scale them\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SelectFromModel(estimator=Lasso(alpha=10, copy_X=True, fit_intercept=True,\n                                max_iter=1000, normalize=False, positive=False,\n                                precompute=False, random_state=None,\n                                selection='random', tol=0.0001,\n                                warm_start=False),\n                max_features=None, norm_order=1, prefit=False, threshold=None)"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "# here, again I will train a Lasso Linear regression and select\n",
    "# the non zero features in one line.\n",
    "# bear in mind that the linear regression object from sklearn does\n",
    "# not allow for regularisation. So If you want to make a regularised\n",
    "# linear regression you need to import specifically \"Lasso\"\n",
    "# that is the l1 version of the linear regression\n",
    "# alpha is the penalisation here, so I set it high in order\n",
    "# to force the algorithm to shrink some coefficients\n",
    "\n",
    "sel_ = SelectFromModel(Lasso(alpha=10, selection='random'))\n",
    "sel_.fit(scaler.transform(X_train.fillna(0)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([False,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True, False,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True])"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "total features: 37\nselected features: 35\nfeatures with coefficients shrank to zero: 2\n"
    }
   ],
   "source": [
    "# make a list with the selected features and print the outputs\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "    np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that Lasso regularisation helps to remove non-important features from the dataset. So, increasing the penalisation will result in increase the number of features removed. Therefore, we need to keep an eye and monitor that we don't set a penalty too high so that to remove even important features, or too low and then not remove non-important features.\n",
    "\n",
    "- If the penalty is too high and important features are removed, we will notice a drop in the performance of the algorithm and then realise that we need to decrease the regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## **3.2 ** Random Forest Importance<a class=\"anchor\" id=\"3.2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "- Random forests are one the most popular machine learning algorithms. They are so successful because they provide in general a good predictive performance, low overfitting and easy interpretability. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable is contributing to the decision.\n",
    "\n",
    "- Random forests consist of 4-12 hundred decision trees, each of them built over a random extraction of the observations from the dataset and a random extraction of the features. Not every tree sees all the features or all the observations, and this guarantees that the trees are de-correlated and therefore less prone to over-fitting. Each tree is also a sequence of yes-no questions based on a single or combination of features. At each node (this is at each question), the three divides the dataset into 2 buckets, each of them hosting observations that are more similar among themselves and different from the ones in the other bucket. Therefore, the importance of each feature is derived by how \"pure\" each of the buckets is.\n",
    "\n",
    "- For classification, the measure of impurity is either the Gini impurity or the information gain/entropy. For regression the measure of impurity is variance. Therefore, when training a tree, it is possible to compute how much each feature decreases the impurity. The more a feature decreases the impurity, the more important the feature is. In random forests, the impurity decrease from each feature can be averaged across trees to determine the final importance of the variable.\n",
    "\n",
    "- To give a better intuition, features that are selected at the top of the trees are in general more important than features that are selected at the end nodes of the trees, as generally the top splits lead to bigger information gains.\n",
    "\n",
    "- Please see my kernel, [Random Forest Classifier + Feature Importance - Section 13. Find important features with Random Forest model](https://www.kaggle.com/prashant111/random-forest-classifier-feature-importance) to know how to find important features using the random forest model.\n",
    "\n",
    "- I will demonstrate this process using the mushroom classification dataset as follows:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../../data/mushrooms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     class cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n0        p         x           s         n       t    p               f   \n1        e         x           s         y       t    a               f   \n2        e         b           s         w       t    l               f   \n3        p         x           y         w       t    p               f   \n4        e         x           s         g       f    n               f   \n...    ...       ...         ...       ...     ...  ...             ...   \n8119     e         k           s         n       f    n               a   \n8120     e         x           s         n       f    n               a   \n8121     e         f           s         n       f    n               a   \n8122     p         k           y         n       f    y               f   \n8123     e         x           s         n       f    n               a   \n\n     gill-spacing gill-size gill-color  ... stalk-surface-below-ring  \\\n0               c         n          k  ...                        s   \n1               c         b          k  ...                        s   \n2               c         b          n  ...                        s   \n3               c         n          n  ...                        s   \n4               w         b          k  ...                        s   \n...           ...       ...        ...  ...                      ...   \n8119            c         b          y  ...                        s   \n8120            c         b          y  ...                        s   \n8121            c         b          n  ...                        s   \n8122            c         n          b  ...                        k   \n8123            c         b          y  ...                        s   \n\n     stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n0                         w                      w         p          w   \n1                         w                      w         p          w   \n2                         w                      w         p          w   \n3                         w                      w         p          w   \n4                         w                      w         p          w   \n...                     ...                    ...       ...        ...   \n8119                      o                      o         p          o   \n8120                      o                      o         p          n   \n8121                      o                      o         p          o   \n8122                      w                      w         p          w   \n8123                      o                      o         p          o   \n\n     ring-number ring-type spore-print-color population habitat  \n0              o         p                 k          s       u  \n1              o         p                 n          n       g  \n2              o         p                 n          n       m  \n3              o         p                 k          s       u  \n4              o         e                 n          a       g  \n...          ...       ...               ...        ...     ...  \n8119           o         p                 b          c       l  \n8120           o         p                 b          v       l  \n8121           o         p                 b          c       l  \n8122           o         e                 w          v       l  \n8123           o         p                 o          c       l  \n\n[8124 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>cap-shape</th>\n      <th>cap-surface</th>\n      <th>cap-color</th>\n      <th>bruises</th>\n      <th>odor</th>\n      <th>gill-attachment</th>\n      <th>gill-spacing</th>\n      <th>gill-size</th>\n      <th>gill-color</th>\n      <th>...</th>\n      <th>stalk-surface-below-ring</th>\n      <th>stalk-color-above-ring</th>\n      <th>stalk-color-below-ring</th>\n      <th>veil-type</th>\n      <th>veil-color</th>\n      <th>ring-number</th>\n      <th>ring-type</th>\n      <th>spore-print-color</th>\n      <th>population</th>\n      <th>habitat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>p</td>\n      <td>x</td>\n      <td>s</td>\n      <td>n</td>\n      <td>t</td>\n      <td>p</td>\n      <td>f</td>\n      <td>c</td>\n      <td>n</td>\n      <td>k</td>\n      <td>...</td>\n      <td>s</td>\n      <td>w</td>\n      <td>w</td>\n      <td>p</td>\n      <td>w</td>\n      <td>o</td>\n      <td>p</td>\n      <td>k</td>\n      <td>s</td>\n      <td>u</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>e</td>\n      <td>x</td>\n      <td>s</td>\n      <td>y</td>\n      <td>t</td>\n      <td>a</td>\n      <td>f</td>\n      <td>c</td>\n      <td>b</td>\n      <td>k</td>\n      <td>...</td>\n      <td>s</td>\n      <td>w</td>\n      <td>w</td>\n      <td>p</td>\n      <td>w</td>\n      <td>o</td>\n      <td>p</td>\n      <td>n</td>\n      <td>n</td>\n      <td>g</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>e</td>\n      <td>b</td>\n      <td>s</td>\n      <td>w</td>\n      <td>t</td>\n      <td>l</td>\n      <td>f</td>\n      <td>c</td>\n      <td>b</td>\n      <td>n</td>\n      <td>...</td>\n      <td>s</td>\n      <td>w</td>\n      <td>w</td>\n      <td>p</td>\n      <td>w</td>\n      <td>o</td>\n      <td>p</td>\n      <td>n</td>\n      <td>n</td>\n      <td>m</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>p</td>\n      <td>x</td>\n      <td>y</td>\n      <td>w</td>\n      <td>t</td>\n      <td>p</td>\n      <td>f</td>\n      <td>c</td>\n      <td>n</td>\n      <td>n</td>\n      <td>...</td>\n      <td>s</td>\n      <td>w</td>\n      <td>w</td>\n      <td>p</td>\n      <td>w</td>\n      <td>o</td>\n      <td>p</td>\n      <td>k</td>\n      <td>s</td>\n      <td>u</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>e</td>\n      <td>x</td>\n      <td>s</td>\n      <td>g</td>\n      <td>f</td>\n      <td>n</td>\n      <td>f</td>\n      <td>w</td>\n      <td>b</td>\n      <td>k</td>\n      <td>...</td>\n      <td>s</td>\n      <td>w</td>\n      <td>w</td>\n      <td>p</td>\n      <td>w</td>\n      <td>o</td>\n      <td>e</td>\n      <td>n</td>\n      <td>a</td>\n      <td>g</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8119</th>\n      <td>e</td>\n      <td>k</td>\n      <td>s</td>\n      <td>n</td>\n      <td>f</td>\n      <td>n</td>\n      <td>a</td>\n      <td>c</td>\n      <td>b</td>\n      <td>y</td>\n      <td>...</td>\n      <td>s</td>\n      <td>o</td>\n      <td>o</td>\n      <td>p</td>\n      <td>o</td>\n      <td>o</td>\n      <td>p</td>\n      <td>b</td>\n      <td>c</td>\n      <td>l</td>\n    </tr>\n    <tr>\n      <th>8120</th>\n      <td>e</td>\n      <td>x</td>\n      <td>s</td>\n      <td>n</td>\n      <td>f</td>\n      <td>n</td>\n      <td>a</td>\n      <td>c</td>\n      <td>b</td>\n      <td>y</td>\n      <td>...</td>\n      <td>s</td>\n      <td>o</td>\n      <td>o</td>\n      <td>p</td>\n      <td>n</td>\n      <td>o</td>\n      <td>p</td>\n      <td>b</td>\n      <td>v</td>\n      <td>l</td>\n    </tr>\n    <tr>\n      <th>8121</th>\n      <td>e</td>\n      <td>f</td>\n      <td>s</td>\n      <td>n</td>\n      <td>f</td>\n      <td>n</td>\n      <td>a</td>\n      <td>c</td>\n      <td>b</td>\n      <td>n</td>\n      <td>...</td>\n      <td>s</td>\n      <td>o</td>\n      <td>o</td>\n      <td>p</td>\n      <td>o</td>\n      <td>o</td>\n      <td>p</td>\n      <td>b</td>\n      <td>c</td>\n      <td>l</td>\n    </tr>\n    <tr>\n      <th>8122</th>\n      <td>p</td>\n      <td>k</td>\n      <td>y</td>\n      <td>n</td>\n      <td>f</td>\n      <td>y</td>\n      <td>f</td>\n      <td>c</td>\n      <td>n</td>\n      <td>b</td>\n      <td>...</td>\n      <td>k</td>\n      <td>w</td>\n      <td>w</td>\n      <td>p</td>\n      <td>w</td>\n      <td>o</td>\n      <td>e</td>\n      <td>w</td>\n      <td>v</td>\n      <td>l</td>\n    </tr>\n    <tr>\n      <th>8123</th>\n      <td>e</td>\n      <td>x</td>\n      <td>s</td>\n      <td>n</td>\n      <td>f</td>\n      <td>n</td>\n      <td>a</td>\n      <td>c</td>\n      <td>b</td>\n      <td>y</td>\n      <td>...</td>\n      <td>s</td>\n      <td>o</td>\n      <td>o</td>\n      <td>p</td>\n      <td>o</td>\n      <td>o</td>\n      <td>p</td>\n      <td>o</td>\n      <td>c</td>\n      <td>l</td>\n    </tr>\n  </tbody>\n</table>\n<p>8124 rows × 23 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare feature vector and target variable\n",
    "X = df.drop(['class'], axis = 1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "X = pd.get_dummies(X, prefix_sep='_')\n",
    "y = LabelEncoder().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize feature vector\n",
    "X2 = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# instantiate the classifier with n_estimators = 100\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n                       warm_start=False)"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "# fit the classifier to the training set\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test set\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Importance**\n",
    "\n",
    "- Decision Trees models which are based on ensembles (eg. Extra Trees and Random Forest) can be used to rank the importance of the different features. Knowing which features our model is giving most importance can be of vital importance to understand how our model is making it’s predictions (therefore making it more explainable). At the same time, we can get rid of the features which do not bring any benefit to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we know which features are most important in the Random Forest model, we can train our model just using these features. \n",
    "\n",
    "- I have implemented this in the kernel - [Random Forest Classifier + Feature Importance : Section 15 - Build the Random Forest model on selected features](https://www.kaggle.com/prashant111/random-forest-classifier-feature-importance). It resulted in improved accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([2.34210965e-03, 9.55828202e-05, 8.95315791e-04, 5.43635904e-05,\n       1.03560698e-04, 8.73953925e-04, 3.36171699e-03, 2.48295146e-04,\n       7.46455150e-03, 1.49709292e-03, 1.16258128e-03, 6.56582068e-04,\n       5.36400581e-04, 1.22994475e-03, 1.23191547e-03, 1.06111338e-03,\n       4.39957059e-04, 3.45992841e-05, 2.81252547e-03, 5.01259882e-03,\n       2.52459526e-02, 1.94944283e-02, 6.13728694e-03, 6.16624040e-03,\n       6.06433684e-02, 4.77200073e-03, 9.58473652e-04, 1.41310186e-01,\n       1.37125929e-02, 4.61934426e-03, 6.97179251e-03, 9.95412984e-04,\n       2.34992935e-04, 1.28427620e-02, 1.41487298e-02, 7.16585669e-02,\n       5.83976304e-02, 4.69392870e-02, 9.99417474e-06, 1.14368133e-03,\n       7.58609796e-04, 1.51031018e-04, 1.08832730e-03, 1.26888344e-04,\n       3.00967718e-04, 2.40433992e-03, 1.20609096e-03, 1.65217528e-03,\n       2.22501146e-04, 1.34404957e-02, 1.42614450e-02, 9.27534795e-03,\n       2.11468821e-02, 7.02677688e-03, 1.27413113e-02, 2.94275119e-04,\n       2.90081155e-03, 5.18788932e-02, 1.67389849e-02, 8.78117680e-05,\n       3.78113649e-03, 4.80436129e-02, 1.42340437e-02, 5.66372356e-03,\n       3.51317683e-03, 6.28653346e-04, 9.64740061e-05, 1.24390414e-03,\n       1.12817935e-03, 2.55121703e-04, 9.36634935e-04, 4.37574001e-03,\n       1.70746129e-04, 3.05530423e-03, 8.78790378e-04, 5.38181436e-04,\n       3.37810548e-03, 9.94334591e-04, 6.52313285e-04, 5.46519215e-05,\n       7.06713584e-03, 1.05740630e-03, 0.00000000e+00, 4.36378871e-06,\n       3.94087012e-06, 1.08491426e-03, 3.38790171e-04, 8.03250972e-04,\n       5.80913352e-03, 6.54116345e-03, 4.46605995e-03, 1.00329931e-03,\n       1.51409207e-02, 1.46883703e-03, 2.65162694e-02, 0.00000000e+00,\n       5.32339140e-02, 5.87156823e-03, 7.04830607e-03, 1.50632116e-04,\n       7.97661126e-03, 2.21058599e-03, 1.05316119e-02, 0.00000000e+00,\n       5.31958772e-04, 2.34086634e-03, 2.74714638e-03, 3.03172832e-03,\n       1.96217890e-02, 3.99268013e-03, 5.45852800e-03, 9.45045509e-03,\n       1.07472862e-03, 3.29921059e-03, 4.23904770e-03, 5.51179402e-03,\n       1.52707110e-03])"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x1a23ef2250>"
     },
     "metadata": {},
     "execution_count": 61
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 800x640 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"465.958125pt\" version=\"1.1\" viewBox=\"0 0 712.040625 465.958125\" width=\"712.040625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 465.958125 \nL 712.040625 465.958125 \nL 712.040625 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 146.840625 442.08 \nL 704.840625 442.08 \nL 704.840625 7.2 \nL 146.840625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p4513b921db)\" d=\"M 146.840625 431.208 \nL 678.269196 431.208 \nL 678.269196 409.464 \nL 146.840625 409.464 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p4513b921db)\" d=\"M 146.840625 387.72 \nL 416.328699 387.72 \nL 416.328699 365.976 \nL 146.840625 365.976 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p4513b921db)\" d=\"M 146.840625 344.232 \nL 374.903581 344.232 \nL 374.903581 322.488 \nL 146.840625 322.488 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p4513b921db)\" d=\"M 146.840625 300.744 \nL 366.45798 300.744 \nL 366.45798 279 \nL 146.840625 279 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p4513b921db)\" d=\"M 146.840625 257.256 \nL 347.038669 257.256 \nL 347.038669 235.512 \nL 146.840625 235.512 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p4513b921db)\" d=\"M 146.840625 213.768 \nL 341.942811 213.768 \nL 341.942811 192.024 \nL 146.840625 192.024 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p4513b921db)\" d=\"M 146.840625 170.28 \nL 327.519381 170.28 \nL 327.519381 148.536 \nL 146.840625 148.536 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p4513b921db)\" d=\"M 146.840625 126.792 \nL 323.366316 126.792 \nL 323.366316 105.048 \nL 146.840625 105.048 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p4513b921db)\" d=\"M 146.840625 83.304 \nL 246.560989 83.304 \nL 246.560989 61.56 \nL 146.840625 61.56 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p4513b921db)\" d=\"M 146.840625 39.816 \nL 241.783679 39.816 \nL 241.783679 18.072 \nL 146.840625 18.072 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m83b69a63fb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.840625\" xlink:href=\"#m83b69a63fb\" y=\"442.08\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.00 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(135.707812 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"222.0551\" xlink:href=\"#m83b69a63fb\" y=\"442.08\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.02 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(210.922287 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"297.269575\" xlink:href=\"#m83b69a63fb\" y=\"442.08\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.04 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(286.136762 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"372.48405\" xlink:href=\"#m83b69a63fb\" y=\"442.08\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.06 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(361.351237 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"447.698525\" xlink:href=\"#m83b69a63fb\" y=\"442.08\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.08 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(436.565712 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"522.913\" xlink:href=\"#m83b69a63fb\" y=\"442.08\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(511.780187 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"598.127474\" xlink:href=\"#m83b69a63fb\" y=\"442.08\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.12 -->\n      <g transform=\"translate(586.994662 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"673.341949\" xlink:href=\"#m83b69a63fb\" y=\"442.08\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.14 -->\n      <g transform=\"translate(662.209137 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"me4f6e377f8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.840625\" xlink:href=\"#me4f6e377f8\" y=\"420.336\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- odor_n -->\n      <defs>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <g transform=\"translate(105.80625 424.135219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"61.181641\" xlink:href=\"#DejaVuSans-100\"/>\n       <use x=\"124.658203\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"185.839844\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"226.953125\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"276.953125\" xlink:href=\"#DejaVuSans-110\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.840625\" xlink:href=\"#me4f6e377f8\" y=\"376.848\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- gill-size_b -->\n      <defs>\n       <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 4.890625 31.390625 \nL 31.203125 31.390625 \nL 31.203125 23.390625 \nL 4.890625 23.390625 \nz\n\" id=\"DejaVuSans-45\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       <path d=\"M 5.515625 54.6875 \nL 48.1875 54.6875 \nL 48.1875 46.484375 \nL 14.40625 7.171875 \nL 48.1875 7.171875 \nL 48.1875 0 \nL 4.296875 0 \nL 4.296875 8.203125 \nL 38.09375 47.515625 \nL 5.515625 47.515625 \nz\n\" id=\"DejaVuSans-122\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n      </defs>\n      <g transform=\"translate(90.8125 380.647219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-103\"/>\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"91.259766\" xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"119.042969\" xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"146.826172\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"182.910156\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"235.009766\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"262.792969\" xlink:href=\"#DejaVuSans-122\"/>\n       <use x=\"315.283203\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"376.806641\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"426.806641\" xlink:href=\"#DejaVuSans-98\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.840625\" xlink:href=\"#me4f6e377f8\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- odor_f -->\n      <defs>\n       <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      </defs>\n      <g transform=\"translate(108.623437 337.159219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"61.181641\" xlink:href=\"#DejaVuSans-100\"/>\n       <use x=\"124.658203\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"185.839844\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"226.953125\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"276.953125\" xlink:href=\"#DejaVuSans-102\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.840625\" xlink:href=\"#me4f6e377f8\" y=\"289.872\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- gill-size_n -->\n      <g transform=\"translate(90.823437 293.671219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-103\"/>\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"91.259766\" xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"119.042969\" xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"146.826172\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"182.910156\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"235.009766\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"262.792969\" xlink:href=\"#DejaVuSans-122\"/>\n       <use x=\"315.283203\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"376.806641\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"426.806641\" xlink:href=\"#DejaVuSans-110\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.840625\" xlink:href=\"#me4f6e377f8\" y=\"246.384\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- spore-print-color_h -->\n      <defs>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <g transform=\"translate(45.229687 250.183219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-112\"/>\n       <use x=\"115.576172\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"176.757812\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"217.839844\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"279.363281\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"315.447266\" xlink:href=\"#DejaVuSans-112\"/>\n       <use x=\"378.923828\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"420.037109\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"447.820312\" xlink:href=\"#DejaVuSans-110\"/>\n       <use x=\"511.199219\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"550.408203\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"586.492188\" xlink:href=\"#DejaVuSans-99\"/>\n       <use x=\"641.472656\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"702.654297\" xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"730.4375\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"791.619141\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"832.732422\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"882.732422\" xlink:href=\"#DejaVuSans-104\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.840625\" xlink:href=\"#me4f6e377f8\" y=\"202.896\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- stalk-surface-above-ring_k -->\n      <defs>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 31.109375 \nL 44.921875 54.6875 \nL 56.390625 54.6875 \nL 27.390625 29.109375 \nL 57.625 0 \nL 45.90625 0 \nL 18.109375 26.703125 \nL 18.109375 0 \nL 9.078125 0 \nz\n\" id=\"DejaVuSans-107\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      </defs>\n      <g transform=\"translate(7.2 206.695219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"91.308594\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"152.587891\" xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"238.28125\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"274.365234\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"326.464844\" xlink:href=\"#DejaVuSans-117\"/>\n       <use x=\"389.84375\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"430.957031\" xlink:href=\"#DejaVuSans-102\"/>\n       <use x=\"466.162109\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"527.441406\" xlink:href=\"#DejaVuSans-99\"/>\n       <use x=\"582.421875\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"643.945312\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"680.029297\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"741.308594\" xlink:href=\"#DejaVuSans-98\"/>\n       <use x=\"804.785156\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"865.966797\" xlink:href=\"#DejaVuSans-118\"/>\n       <use x=\"925.146484\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"986.669922\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"1022.753906\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"1063.867188\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"1091.650391\" xlink:href=\"#DejaVuSans-110\"/>\n       <use x=\"1155.029297\" xlink:href=\"#DejaVuSans-103\"/>\n       <use x=\"1218.505859\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1268.505859\" xlink:href=\"#DejaVuSans-107\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.840625\" xlink:href=\"#me4f6e377f8\" y=\"159.408\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- stalk-surface-below-ring_k -->\n      <defs>\n       <path d=\"M 4.203125 54.6875 \nL 13.1875 54.6875 \nL 24.421875 12.015625 \nL 35.59375 54.6875 \nL 46.1875 54.6875 \nL 57.421875 12.015625 \nL 68.609375 54.6875 \nL 77.59375 54.6875 \nL 63.28125 0 \nL 52.6875 0 \nL 40.921875 44.828125 \nL 29.109375 0 \nL 18.5 0 \nz\n\" id=\"DejaVuSans-119\"/>\n      </defs>\n      <g transform=\"translate(8.290625 163.207219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"91.308594\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"152.587891\" xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"238.28125\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"274.365234\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"326.464844\" xlink:href=\"#DejaVuSans-117\"/>\n       <use x=\"389.84375\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"430.957031\" xlink:href=\"#DejaVuSans-102\"/>\n       <use x=\"466.162109\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"527.441406\" xlink:href=\"#DejaVuSans-99\"/>\n       <use x=\"582.421875\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"643.945312\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"680.029297\" xlink:href=\"#DejaVuSans-98\"/>\n       <use x=\"743.505859\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"805.029297\" xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"832.8125\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"893.994141\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"975.78125\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"1011.865234\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"1052.978516\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"1080.761719\" xlink:href=\"#DejaVuSans-110\"/>\n       <use x=\"1144.140625\" xlink:href=\"#DejaVuSans-103\"/>\n       <use x=\"1207.617188\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1257.617188\" xlink:href=\"#DejaVuSans-107\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.840625\" xlink:href=\"#me4f6e377f8\" y=\"115.92\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- gill-color_b -->\n      <g transform=\"translate(85.576562 119.719219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-103\"/>\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"91.259766\" xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"119.042969\" xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"146.826172\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"182.910156\" xlink:href=\"#DejaVuSans-99\"/>\n       <use x=\"237.890625\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"299.072266\" xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"326.855469\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"388.037109\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"429.150391\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"479.150391\" xlink:href=\"#DejaVuSans-98\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.840625\" xlink:href=\"#me4f6e377f8\" y=\"72.432\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- ring-type_p -->\n      <defs>\n       <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      </defs>\n      <g transform=\"translate(82.96875 76.231219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"41.113281\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"68.896484\" xlink:href=\"#DejaVuSans-110\"/>\n       <use x=\"132.275391\" xlink:href=\"#DejaVuSans-103\"/>\n       <use x=\"195.751953\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"231.835938\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"271.044922\" xlink:href=\"#DejaVuSans-121\"/>\n       <use x=\"330.224609\" xlink:href=\"#DejaVuSans-112\"/>\n       <use x=\"393.701172\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"455.224609\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"505.224609\" xlink:href=\"#DejaVuSans-112\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.840625\" xlink:href=\"#me4f6e377f8\" y=\"28.944\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- bruises_f -->\n      <g transform=\"translate(95.173437 32.743219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-98\"/>\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"104.589844\" xlink:href=\"#DejaVuSans-117\"/>\n       <use x=\"167.96875\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"195.751953\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"247.851562\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"309.375\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"361.474609\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"411.474609\" xlink:href=\"#DejaVuSans-102\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 146.840625 442.08 \nL 146.840625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 704.840625 442.08 \nL 704.840625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 146.840625 442.08 \nL 704.840625 442.08 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 146.840625 7.2 \nL 704.840625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4513b921db\">\n   <rect height=\"434.88\" width=\"558\" x=\"146.840625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# visualize feature importance\n",
    "\n",
    "plt.figure(num=None, figsize=(10,8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "feat_importances = pd.Series(clf.feature_importances_, index= X.columns)\n",
    "\n",
    "feat_importances.nlargest(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. How to choose the right feature selection method** <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- We can see that there are lot of feature selection techniques available. \n",
    "\n",
    "- The following graphic will serve as a guide on how to choose a feature selection method:-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![How to Choose a Feature Selection Method](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/11/How-to-Choose-Feature-Selection-Methods-For-Machine-Learning.png)\n",
    "\n",
    "\n",
    "## **Image Source : Machine Learning Mastery**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Numerical Input, Numerical Output**\n",
    "\n",
    "- This is a regression predictive modeling problem with numerical input variables.\n",
    "\n",
    "- The most common techniques are to use a correlation coefficient, such as Pearson’s for a linear correlation, or rank-based methods for a nonlinear correlation.\n",
    "\n",
    "- The tests emplyed are as follows:-\n",
    "\n",
    "  - Pearson’s correlation coefficient (linear).\n",
    "  - Spearman’s rank coefficient (nonlinear)\n",
    "  \n",
    "  \n",
    "## **Numerical Input, Categorical Output**\n",
    "\n",
    "- This is a classification predictive modeling problem with numerical input variables.\n",
    "\n",
    "- This might be the most common example of a classification problem,\n",
    "\n",
    "- Again, the most common techniques are correlation based, although in this case, they must take the categorical target into account.\n",
    "\n",
    "- We can employ the following tests as follows:-\n",
    "\n",
    "  - ANOVA correlation coefficient (linear).\n",
    "  - Kendall’s rank coefficient (nonlinear).\n",
    "  \n",
    "- Kendall does assume that the categorical variable is ordinal.\n",
    "\n",
    "\n",
    "## **Categorical Input, Numerical Output**\n",
    "\n",
    "- This is a regression predictive modeling problem with categorical input variables.\n",
    "\n",
    "- This is a strange example of a regression problem (e.g. we will not encounter it often).\n",
    "\n",
    "- We can use the same “Numerical Input, Categorical Output” methods (described above), but in reverse.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **Categorical Input, Categorical Output**\n",
    "\n",
    "- This is a classification predictive modeling problem with categorical input variables.\n",
    "\n",
    "- The most common correlation measure for categorical data is the chi-squared test. We can also use mutual information (information gain) from the field of information theory.\n",
    "\n",
    "- The following tests can be employed in this case -\n",
    "\n",
    "  - Chi-Squared test (contingency tables).\n",
    "  - Mutual Information.\n",
    "  \n",
    "In fact, mutual information is a powerful method that may prove useful for both categorical and numerical data, e.g. it is agnostic to the data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Tips and Tricks for Feature Selection** <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- In this section, we provide some additional considerations when using filter-based feature selection.\n",
    "\n",
    "\n",
    "## **Correlation Statistics**\n",
    "\n",
    "- The scikit-learn library provides an implementation of most of the useful statistical measures.\n",
    "\n",
    "- For example:\n",
    "\n",
    "   - Pearson’s Correlation Coefficient: [f_regression()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)\n",
    "   - ANOVA: [f_classif()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html)\n",
    "   - Chi-Squared: [chi2()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)\n",
    "   - Mutual Information: [mutual_info_classif()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) and [mutual_info_regression()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html).\n",
    "   \n",
    "   \n",
    "- Also, the SciPy library provides an implementation of many more statistics, such as Kendall’s tau (kendalltau) and Spearman’s rank correlation (spearmanr).\n",
    "\n",
    "\n",
    "## **Selection Method**\n",
    "\n",
    "- The scikit-learn library also provides many different filtering methods once statistics have been calculated for each input variable with the target.\n",
    "\n",
    "- Two of the more popular methods include:\n",
    "\n",
    "    - Select the top k variables: [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "    - Select the top percentile variables: [SelectPercentile](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html)\n",
    "    \n",
    "\n",
    "\n",
    "## **Transform Variables**\n",
    "\n",
    "- We can consider transforming the variables in order to access different statistical methods. For example, we can transform a categorical variable to ordinal, even if it is not, and see if any interesting results come out.\n",
    "\n",
    "- We can also make a numerical variable discrete (e.g. bins); try categorical-based measures.\n",
    "\n",
    "- Some statistical measures assume properties of the variables, such as Pearson’s that assumes a Gaussian probability distribution to the observations and a linear relationship. You can transform the data to meet the expectations of the test and try the test regardless of the expectations and compare results.\n",
    "\n",
    "\n",
    "## **What Is the Best Method?**\n",
    "\n",
    "- There is no best feature selection method. Just like there is no best set of input variables or best machine learning algorithm. \n",
    "\n",
    "- Instead, we must discover what works best for your specific problem using careful systematic experimentation.\n",
    "\n",
    "- We should try a range of different models fit on different subsets of features chosen via different statistical measures and discover what works best for your specific problem.\n",
    "\n",
    "\n",
    "## **4 best ways of Feature Selection**\n",
    "\n",
    "- The 4 practical ways of feature selection which yield best results are as follows:-\n",
    "\n",
    "    1. SelectKBest\n",
    "    2. Recursive Feature Elimination\n",
    "    3. Correlation-matrix with heatmap\n",
    "    4. Random-Forest Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# **6. References** <a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- The work done in this kernel is inspired from the following courses and websites:-\n",
    "\n",
    "\n",
    "  1. [Feature Selection for Machine Learning](https://www.udemy.com/course/feature-selection-for-machine-learning/) by Soledad Galli\n",
    "\n",
    "  2. [Analytics Vidhya article on Feature Selection](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/)\n",
    "\n",
    "  3. https://en.wikipedia.org/wiki/Feature_selection\n",
    "\n",
    "  4. https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to Top](#0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}