{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Data Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type of Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MCAR\n",
    "* MAR\n",
    "* MNAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Listwise Deletion: removing the entire record of the data that contains one or more missing data. \n",
    "\n",
    "    * Disadvantages — Statistical power relies on a high sample size. In smaller sets of data, listwise deletion can reduce sample size. Unless you are sure that the record is definitely not MNAR, this technique may introduce bias into the dataset.\n",
    "    \n",
    "* Pairwise Deletion: It is the method that uses the correlation between pairs of variables to maximize data available on an analysis by analysis basis.\n",
    "    * Disadvantages — It’s difficult to interpret parts of your model due to the fact that there are different numbers of observations contributing to different parts of your model.\n",
    "- Dropping Variables: It is the method to drop a variable if 60% of the data is missing. It’s difficult to know how your dropped variable may affect other variables inside the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Mean, Mode Median\n",
    "\n",
    "The disadvantage of Mean, Median, Mode Imputations \n",
    "- It reduces the variance of the imputed variables. \n",
    "- It also shrinks the standard error, which invalidates most hypothesis tests and the calculation of confidence interval. \n",
    "- It disregards the correlations between variables. It can over-represent and under-represent certain data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Column_Name.fillna(df.Column_Name.mean(), inplace=True)\n",
    "df.Column_Name.fillna(df.Column_Name.median(), inplace=True)\n",
    "df.Column_Name.fillna(df.Column_Name.mode(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Logistic Regression\n",
    "\n",
    "Disadvantages of Logistic Regression:\n",
    "- prone to overconfidence or overfitting due to the fact of overstating the accuracy of its predictions.\n",
    "- tend to underperform when there are multiple or nonlinear decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "imp=Imputer(missing_values=\"NaN\", strategy=\"mean\", axis=0)\n",
    "logmodel = LogisticRegression()\n",
    "steps=[('imputation',imp),('logistic_regression',logmodel)]\n",
    "pipeline=Pipeline(steps)\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "pipeline.fit(X_train, Y_train)\n",
    "y_pred=pipeline.predict(X_test)\n",
    "pipeline.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages of Linear Regression:\n",
    "- the standard error is deflated.\n",
    "- must have a linear relationship between x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearModel\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "imp=Imputer(missing_values=\"NaN\", strategy=\"mean\", axis=0)\n",
    "linmodel = LinearModel()\n",
    "steps=[('imputation',imp),('linear_regression',linmodel)]\n",
    "pipeline=Pipeline(steps)\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "pipeline.fit(X_train, Y_train)\n",
    "y_pred=pipeline.predict(X_test)\n",
    "pipeline.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- KNN\n",
    " This is a model that’s widely used for missing data imputation. The reason it is widely used is due to the fact that it can handle both continuous data and categorical data.\n",
    " This model is a non-parametric method that classifies the data to its nearest heavily weighted neighbor.\n",
    " \n",
    " Disadvantages of KNN\n",
    "- time-consuming on larger datasets\n",
    "- on high dimensional data, accuracy can be severely degraded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "k_range=range(1,26)\n",
    "for k in k_range:\n",
    "    imp=Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    knn=KNeighborsClassifier(n_neighbors=k)\n",
    "    steps=[('imputation',imp),('K-Nearest Neighbor',knn)]\n",
    "    pipeline=Pipeline(steps)\n",
    "    X_train, X_test, Y_train, Y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "    y_pred=pipeline.predict(X_test)\n",
    "    pipeline.score(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
